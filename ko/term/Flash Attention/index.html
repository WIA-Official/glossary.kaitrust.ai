<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flash Attention | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° ì•Œê³ ë¦¬ì¦˜">
    <meta name="keywords" content="Flash Attention, , AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/Flash%20Attention/">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/Flash%20Attention/">
    <meta property="og:title" content="Flash Attention | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° ì•Œê³ ë¦¬ì¦˜">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Flash Attention | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° ì•Œê³ ë¦¬ì¦˜">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <!-- Structured Data (JSON-LD) -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "DefinedTerm",
        "name": "Flash Attention",
        "description": "ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° ì•Œê³ ë¦¬ì¦˜",
        "inDefinedTermSet": {
            "@type": "DefinedTermSet",
            "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „",
            "url": "https://glossary.kaitrust.ai/"
        }
    }
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Common CSS -->
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 120px 2rem 4rem;
            position: relative;
            z-index: 1;
        }
        .breadcrumb {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            flex-wrap: wrap;
        }
        .breadcrumb a {
            color: #64748b;
            text-decoration: none;
            transition: color 0.2s;
        }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header {
            background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6));
            border: 1px solid rgba(168, 85, 247, 0.2);
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 2rem;
            position: relative;
            overflow: hidden;
        }
        .term-category-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: rgba(168, 85, 247, 0.2);
            border-radius: 20px;
            font-size: 0.85rem;
            color: #a855f7;
            margin-bottom: 1rem;
        }
        .term-title {
            font-family: 'Orbitron', sans-serif;
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: linear-gradient(135deg, #ffffff, #a855f7);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .term-english {
            font-size: 1.2rem;
            color: #94a3b8;
            margin-bottom: 1.5rem;
        }
        .term-description {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #e2e8f0;
        }
        .term-actions {
            display: flex;
            gap: 1rem;
            margin-top: 2rem;
            flex-wrap: wrap;
        }
        .term-action-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.75rem 1.5rem;
            border-radius: 12px;
            font-size: 0.9rem;
            text-decoration: none;
            transition: all 0.3s;
        }
        .btn-primary {
            background: linear-gradient(135deg, #a855f7, #6366f1);
            color: white;
        }
        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3);
        }
        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            color: #e2e8f0;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.2);
        }
        .related-section {
            margin-top: 3rem;
        }
        .related-title {
            font-size: 1.3rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: #f1f5f9;
        }
        @media (max-width: 768px) {
            .term-detail-container { padding: 100px 1rem 2rem; }
            .term-detail-header { padding: 2rem 1.5rem; }
            .term-title { font-size: 1.8rem; }
        }
        /* ê³ ë„í™” ì„¹ì…˜ ìŠ¤íƒ€ì¼ */
        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .code-tabs { display: flex; gap: 0.5rem; margin-bottom: 1rem; }
        .code-tab { padding: 0.5rem 1rem; background: rgba(255,255,255,0.05); border: 1px solid rgba(255,255,255,0.1); border-radius: 8px; color: #94a3b8; cursor: pointer; }
        .code-tab.active { background: rgba(168,85,247,0.2); border-color: #a855f7; color: #a855f7; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168,85,247,0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0,0,0,0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0,0,0,0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168,85,247,0.1); border: 1px solid rgba(168,85,247,0.3); border-radius: 20px; color: #a855f7; text-decoration: none; }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0,0,0,0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260130000138">

</head>
<body>
    <!-- Particle Background -->
    <div class="particle-container" id="particles"></div>

    <!-- Header -->
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <!-- Breadcrumb -->
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a>
            <span>â€º</span>
            <span class="current">Flash Attention</span>
        </nav>

        <!-- Term Header -->
        <article class="term-detail-header">
            <div class="term-category-badge">
                <span>ğŸ¤–</span>
                <span>AI/ML</span>
            </div>
            <h1 class="term-title">Flash Attention</h1>
            
            <div class="term-description">
                <p>ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° ì•Œê³ ë¦¬ì¦˜</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">
                    ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°
                </a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">
                    ğŸ¤– AI/ML ë”ë³´ê¸°
                </a>
            </div>
        </article>

        <!-- ìƒì„¸ ì„¤ëª… -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>Flash Attentionì€ Transformerì˜ self-attention ì—°ì‚°ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ê¸°ì¡´ attentionì€ ì‹œí€€ìŠ¤ ê¸¸ì´ì˜ ì œê³±(O(n^2))ì— ë¹„ë¡€í•˜ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, Flash Attentionì€ tilingê³¼ recomputation ê¸°ë²•ìœ¼ë¡œ ì´ë¥¼ ì„ í˜•(O(n))ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.</p>
                <p>2022ë…„ Stanfordì˜ Tri Dao ë“±ì´ ë°œí‘œí•œ Flash Attentionì€ GPU ë©”ëª¨ë¦¬ ê³„ì¸µ(HBM vs SRAM)ì„ ê³ ë ¤í•œ IO-aware ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. GPUì˜ ê³ ì† SRAMì—ì„œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê³ , ëŠë¦° HBM ì ‘ê·¼ì„ ìµœì†Œí™”í•˜ì—¬ ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë™ì‹œì— ê°œì„ í–ˆìŠµë‹ˆë‹¤.</p>
                <p>í•µì‹¬ ì•„ì´ë””ì–´ëŠ” attention matrix ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ ì•Šê³ , ì‘ì€ ë¸”ë¡(tile) ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. softmaxì˜ ë¶„ìì™€ ë¶„ëª¨ë¥¼ ì˜¨ë¼ì¸ìœ¼ë¡œ ëˆ„ì  ê³„ì‚°í•˜ê³ , ì—­ì „íŒŒ ì‹œ í•„ìš”í•œ ê°’ì€ ì¬ê³„ì‚°(recomputation)í•©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ ì •í™•í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ë©´ì„œë„ 2~4ë°° ë¹ ë¦…ë‹ˆë‹¤.</p>
                <p>í˜„ì¬ GPT-4, Llama, Claude ë“± ê±°ì˜ ëª¨ë“  ëŒ€í˜• LLMì´ Flash Attentionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ 2Kì—ì„œ 10ë°°, 4Kì—ì„œ 20ë°°ì˜ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ê°€ ìˆì–´, 128K ì´ìƒì˜ ê¸´ contextë¥¼ ì²˜ë¦¬í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. Flash Attention 3ì€ H100 GPUì—ì„œ 1.5~2ë°° ì¶”ê°€ ì†ë„ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.</p>
            </div>
        </section>

        <!-- ì½”ë“œ ì˜ˆì œ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-tabs">
                <button class="code-tab active" data-lang="python">Python</button>
            </div>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python">import torch
from torch.nn.functional import scaled_dot_product_attention

# PyTorch 2.0+ì—ì„œ Flash Attention ìë™ ì‚¬ìš©
# CUDA 11.6+, Ampere GPU (A100, RTX 30xx) ì´ìƒ í•„ìš”

# ì…ë ¥ í…ì„œ ì¤€ë¹„ (batch, heads, seq_len, head_dim)
batch_size = 2
num_heads = 8
seq_len = 4096  # ê¸´ ì‹œí€€ìŠ¤
head_dim = 64

query = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float16)
key = torch.randn(batch_size, num_heads, seq_len, head_dim,
                  device='cuda', dtype=torch.float16)
value = torch.randn(batch_size, num_heads, seq_len, head_dim,
                    device='cuda', dtype=torch.float16)

# scaled_dot_product_attentionì€ ìë™ìœ¼ë¡œ Flash Attention ì‚¬ìš©
# (ì¡°ê±´ ì¶©ì¡± ì‹œ: CUDA, fp16/bf16, ì ì ˆí•œ í…ì„œ shape)
with torch.backends.cuda.sdp_kernel(
    enable_flash=True,      # Flash Attention í™œì„±í™”
    enable_math=False,      # ê¸°ë³¸ ìˆ˜í•™ ì—°ì‚° ë¹„í™œì„±í™”
    enable_mem_efficient=False  # Memory Efficient Attention ë¹„í™œì„±í™”
):
    output = scaled_dot_product_attention(query, key, value)

print(f"Input shape: {query.shape}")
print(f"Output shape: {output.shape}")

# HuggingFace Transformersì—ì„œ Flash Attention 2 ì‚¬ìš©
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    torch_dtype=torch.float16,
    attn_implementation="flash_attention_2",  # Flash Attention 2 ëª…ì‹œ
    device_map="auto"
)

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
print(f"GPU Memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <!-- ì‹¤ë¬´ ëŒ€í™” ì˜ˆì‹œ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ íšŒì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"32K contextë¥¼ ì²˜ë¦¬í•˜ë ¤ë©´ Flash Attentionì´ í•„ìˆ˜ì…ë‹ˆë‹¤. ì¼ë°˜ attentionìœ¼ë¡œëŠ” A100 80GBì—ì„œë„ OOMì´ ë‚˜ëŠ”ë°, Flash Attention 2ë¥¼ ì“°ë©´ ë°°ì¹˜ ì‚¬ì´ì¦ˆë„ ëŠ˜ë¦´ ìˆ˜ ìˆì–´ìš”."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"Flash Attentionì€ attention matrixë¥¼ tile ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ SRAMì—ì„œ ê³„ì‚°í•˜ê³ , HBM ì ‘ê·¼ì„ ìµœì†Œí™”í•˜ëŠ” IO-aware ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. softmaxë¥¼ onlineìœ¼ë¡œ ê³„ì‚°í•´ì„œ ë©”ëª¨ë¦¬ë¥¼ O(n)ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ê¸°ìˆ  í† ë¡ ì—ì„œ</div>
                    <blockquote class="conv-quote">"HuggingFaceì—ì„œ attn_implementation='flash_attention_2' ì˜µì…˜ í•˜ë‚˜ë¡œ ì ìš© ê°€ëŠ¥í•´ìš”. ë‹¤ë§Œ causal maskê°€ ê¸°ë³¸ì´ë¼ encoder-decoder ëª¨ë¸ì—ì„  ì£¼ì˜ê°€ í•„ìš”í•˜ê³ , attention weight ì‹œê°í™”ê°€ ì•ˆ ë˜ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤."</blockquote>
                </div>
            </div>
        </section>

        <!-- ì£¼ì˜ì‚¬í•­ -->
        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>êµ¬í˜• GPUì—ì„œ Flash Attention ì‹œë„</strong>
                        <p>Flash Attentionì€ NVIDIA Ampere(A100, RTX 30xx) ì´ìƒì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤. V100ì´ë‚˜ RTX 20xxì—ì„œëŠ” ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜ fallbackë©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>FP32ë¡œ Flash Attention ì‚¬ìš©</strong>
                        <p>Flash Attentionì€ FP16 ë˜ëŠ” BF16ì—ì„œë§Œ ë™ì‘í•©ë‹ˆë‹¤. FP32 í…ì„œë¥¼ ë„£ìœ¼ë©´ ìë™ìœ¼ë¡œ ëŠë¦° ê¸°ë³¸ attentionìœ¼ë¡œ fallbackë©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ì„¤ì •</strong>
                        <p>PyTorch 2.0+ ì‚¬ìš©, torch.float16 ë˜ëŠ” torch.bfloat16ìœ¼ë¡œ ìºìŠ¤íŒ…, Ampere+ GPU í™•ì¸. HuggingFaceì—ì„œëŠ” attn_implementation="flash_attention_2" ëª…ì‹œì  ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- ê´€ë ¨ ìš©ì–´ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/Self-Attention/" class="related-term-link">Self-Attention</a>
                <a href="/ko/term/Multi-Head%20Attention/" class="related-term-link">Multi-Head Attention</a>
                <a href="/ko/term/KV%20Cache/" class="related-term-link">KV Cache</a>
                <a href="/ko/term/Mixed%20Precision/" class="related-term-link">Mixed Precision</a>
            </div>
        </section>

        <!-- ë” ë°°ìš°ê¸° -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/2205.14135" target="_blank" class="learn-link">ğŸ“„ Flash Attention ì›ë…¼ë¬¸ (arXiv)</a>
                <a href="https://github.com/Dao-AILab/flash-attention" target="_blank" class="learn-link">ğŸ“ Flash Attention GitHub Repository</a>
            </div>
        </section>
    </main>

    <!-- Footer -->
        <div id="kaitrust-footer"></div>

    <!-- Scripts -->
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
    <script>
    function copyCode(btn) {
        const codeBlock = btn.parentElement.querySelector('code');
        navigator.clipboard.writeText(codeBlock.textContent).then(() => {
            btn.textContent = 'âœ… ë³µì‚¬ë¨!';
            setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000);
        });
    }
    </script>
<script src="/glossary/js/term-sections.js?v=20260130000138"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>