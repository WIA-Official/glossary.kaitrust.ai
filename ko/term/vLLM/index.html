<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vLLM (vLLM) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="LLM ì¶”ë¡  ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. PagedAttentionìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”.">
    <meta name="keywords" content="vLLM, LLM ì¶”ë¡ , PagedAttention, ê³ ì„±ëŠ¥ ì¶”ë¡ , AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/vLLM/">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/vLLM/">
    <meta property="og:title" content="vLLM (vLLM) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="LLM ì¶”ë¡  ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. PagedAttentionìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”.">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="vLLM (vLLM) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="LLM ì¶”ë¡  ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. PagedAttentionìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”.">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <!-- Structured Data (JSON-LD) -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "DefinedTerm",
        "name": "vLLM",
        "description": "LLM ì¶”ë¡  ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. PagedAttentionìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”.",
        "inDefinedTermSet": {
            "@type": "DefinedTermSet",
            "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „",
            "url": "https://glossary.kaitrust.ai/"
        }
    }
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Common CSS -->
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 120px 2rem 4rem;
            position: relative;
            z-index: 1;
        }
        .breadcrumb {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            flex-wrap: wrap;
        }
        .breadcrumb a {
            color: #64748b;
            text-decoration: none;
            transition: color 0.2s;
        }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header {
            background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6));
            border: 1px solid rgba(168, 85, 247, 0.2);
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 2rem;
            position: relative;
            overflow: hidden;
        }
        .term-category-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: rgba(168, 85, 247, 0.2);
            border-radius: 20px;
            font-size: 0.85rem;
            color: #a855f7;
            margin-bottom: 1rem;
        }
        .term-title {
            font-family: 'Orbitron', sans-serif;
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: linear-gradient(135deg, #ffffff, #a855f7);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .term-english {
            font-size: 1.2rem;
            color: #94a3b8;
            margin-bottom: 1.5rem;
        }
        .term-description {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #e2e8f0;
        }
        .term-actions {
            display: flex;
            gap: 1rem;
            margin-top: 2rem;
            flex-wrap: wrap;
        }
        .term-action-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.75rem 1.5rem;
            border-radius: 12px;
            font-size: 0.9rem;
            text-decoration: none;
            transition: all 0.3s;
        }
        .btn-primary {
            background: linear-gradient(135deg, #a855f7, #6366f1);
            color: white;
        }
        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3);
        }
        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            color: #e2e8f0;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.2);
        }
        @media (max-width: 768px) {
            .term-detail-container { padding: 100px 1rem 2rem; }
            .term-detail-header { padding: 2rem 1.5rem; }
            .term-title { font-size: 1.8rem; }
        }

        /* ê³ ë„í™” ì„¹ì…˜ ìŠ¤íƒ€ì¼ */
        .term-section {
            background: rgba(15, 23, 42, 0.6);
            border: 1px solid rgba(168, 85, 247, 0.1);
            border-radius: 16px;
            padding: 2rem;
            margin-bottom: 1.5rem;
        }
        .section-title {
            font-size: 1.4rem;
            font-weight: 600;
            color: #f1f5f9;
            margin-bottom: 1.5rem;
            padding-bottom: 0.75rem;
            border-bottom: 1px solid rgba(168, 85, 247, 0.2);
        }
        .section-content p {
            color: #cbd5e1;
            line-height: 1.8;
            margin-bottom: 1rem;
        }
        .section-content p:last-child { margin-bottom: 0; }

        /* ì½”ë“œ ë¸”ë¡ */
        .code-tabs { display: flex; gap: 0.5rem; margin-bottom: 1rem; }
        .code-tab {
            padding: 0.5rem 1rem;
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid rgba(255, 255, 255, 0.1);
            border-radius: 8px;
            color: #94a3b8;
            cursor: pointer;
            transition: all 0.2s;
        }
        .code-tab.active {
            background: rgba(168, 85, 247, 0.2);
            border-color: #a855f7;
            color: #a855f7;
        }
        .code-block {
            position: relative;
            background: #0f172a;
            border-radius: 12px;
            overflow: hidden;
        }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            color: #e2e8f0;
            line-height: 1.6;
        }
        .copy-btn {
            position: absolute;
            top: 0.75rem;
            right: 0.75rem;
            padding: 0.5rem 1rem;
            background: rgba(168, 85, 247, 0.3);
            border: none;
            border-radius: 6px;
            color: #e2e8f0;
            cursor: pointer;
            font-size: 0.85rem;
            transition: all 0.2s;
        }
        .copy-btn:hover { background: rgba(168, 85, 247, 0.5); }

        /* ëŒ€í™” ì˜ˆì‹œ */
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0, 0, 0, 0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote {
            color: #e2e8f0;
            font-style: italic;
            line-height: 1.7;
            margin: 0;
            padding-left: 1rem;
            border-left: 3px solid #a855f7;
        }

        /* ì£¼ì˜ì‚¬í•­ */
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; font-size: 0.95rem; }

        /* ê´€ë ¨ ìš©ì–´ */
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link {
            padding: 0.5rem 1rem;
            background: rgba(168, 85, 247, 0.1);
            border: 1px solid rgba(168, 85, 247, 0.3);
            border-radius: 20px;
            color: #a855f7;
            text-decoration: none;
            font-size: 0.9rem;
            transition: all 0.2s;
        }
        .related-term-link:hover { background: rgba(168, 85, 247, 0.2); transform: translateY(-2px); }

        /* ë” ë°°ìš°ê¸° */
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            padding: 1rem;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 10px;
            color: #00f5ff;
            text-decoration: none;
            transition: all 0.2s;
        }
        .learn-link:hover { background: rgba(0, 0, 0, 0.3); transform: translateX(5px); }

        /* ì„±ëŠ¥ í…Œì´ë¸” */
        .perf-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }
        .perf-table th, .perf-table td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid rgba(168, 85, 247, 0.2);
        }
        .perf-table th {
            background: rgba(168, 85, 247, 0.1);
            color: #a855f7;
            font-weight: 600;
        }
        .perf-table td { color: #cbd5e1; }
        .perf-table tr:hover td { background: rgba(168, 85, 247, 0.05); }
        .perf-note {
            font-size: 0.85rem;
            color: #94a3b8;
            margin-top: 1rem;
            padding: 1rem;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 8px;
            border-left: 3px solid #a855f7;
        }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260129232035">

</head>
<body>
    <!-- Particle Background -->
    <div class="particle-container" id="particles"></div>

    <!-- Header -->
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <!-- Breadcrumb -->
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a>
            <span>â€º</span>
            <span class="current">vLLM</span>
        </nav>

        <!-- Term Header -->
        <article class="term-detail-header">
            <div class="term-category-badge">
                <span>ğŸ¤–</span>
                <span>AI/ML</span>
            </div>
            <h1 class="term-title">vLLM</h1>
            <p class="term-english">Virtual Large Language Model</p>
            <div class="term-description">
                <p>LLM ì¶”ë¡  ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬. PagedAttentionìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ ê·¹ëŒ€í™”.</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">
                    ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°
                </a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">
                    ğŸ¤– AI/ML ë”ë³´ê¸°
                </a>
            </div>
        </article>

        <!-- ìƒì„¸ ì„¤ëª… ì„¹ì…˜ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>vLLMì€ UC Berkeleyì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ë° ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. 2023ë…„ ê³µê°œ ì´í›„ PagedAttentionì´ë¼ëŠ” í˜ì‹ ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê¸°ë²•ìœ¼ë¡œ ê¸°ì¡´ ëŒ€ë¹„ 2~24ë°° ë†’ì€ ì²˜ë¦¬ëŸ‰(throughput)ì„ ë‹¬ì„±í•˜ë©° ì—…ê³„ í‘œì¤€ìœ¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤. OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì„œë²„ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆì–´, í”„ë¡œë•ì…˜ LLM ì„œë¹„ìŠ¤ êµ¬ì¶•ì— í•„ìˆ˜ì ì¸ ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
                <p>vLLMì˜ í•µì‹¬ ê¸°ìˆ ì¸ PagedAttentionì€ ìš´ì˜ì²´ì œì˜ ê°€ìƒ ë©”ëª¨ë¦¬ í˜ì´ì§•ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ LLM ì¶”ë¡ ì—ì„œëŠ” KV ìºì‹œ(Key-Value Cache)ê°€ ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì°¨ì§€í•´ì•¼ í•´ì„œ, ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ì „ í• ë‹¹í•˜ê³  ë‚¨ëŠ” ê³µê°„ì€ ë‚­ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. PagedAttentionì€ KV ìºì‹œë¥¼ ê³ ì • í¬ê¸° ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¹„ì—°ì†ì ìœ¼ë¡œ ì €ì¥í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ë‚­ë¹„ë¥¼ 60~80% ì¤„ì…ë‹ˆë‹¤.</p>
                <p>vLLMì€ Continuous Batchingì„ í†µí•´ ë™ì ìœ¼ë¡œ ìš”ì²­ì„ ë°°ì¹˜í•©ë‹ˆë‹¤. ì „í†µì ì¸ ì •ì  ë°°ì¹­ì€ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ê°€ ëë‚  ë•Œê¹Œì§€ GPUê°€ ëŒ€ê¸°í•´ì•¼ í–ˆì§€ë§Œ, vLLMì€ ì™„ë£Œëœ ìš”ì²­ì„ ì¦‰ì‹œ ì œê±°í•˜ê³  ìƒˆ ìš”ì²­ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ë˜í•œ Tensor Parallelismê³¼ Pipeline Parallelismì„ ì§€ì›í•´ ë‹¤ì¤‘ GPU í™˜ê²½ì—ì„œ ëŒ€í˜• ëª¨ë¸(70B+)ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì„œë¹™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                <p>ì‹¤ë¬´ì—ì„œ vLLMì€ Llama, Mistral, Qwen, Yi ë“± ëŒ€ë¶€ë¶„ì˜ ì˜¤í”ˆì†ŒìŠ¤ LLMì„ ì§€ì›í•˜ë©°, AWQ/GPTQ ì–‘ìí™” ëª¨ë¸ë„ ì„œë¹™ ê°€ëŠ¥í•©ë‹ˆë‹¤. Kubernetes í™˜ê²½ì—ì„œì˜ ë°°í¬, Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ, OpenAI í˜¸í™˜ API ë“± í”„ë¡œë•ì…˜ì— í•„ìš”í•œ ê¸°ëŠ¥ì„ ê¸°ë³¸ ì œê³µí•©ë‹ˆë‹¤. 2025ë…„ í˜„ì¬ H100ì—ì„œ Llama 8B ê¸°ì¤€ 2,300~2,500 tokens/sì˜ ì²˜ë¦¬ëŸ‰ì„ ê¸°ë¡í•˜ë©°, Ollama ëŒ€ë¹„ 20ë°° ì´ìƒì˜ ì²˜ë¦¬ëŸ‰ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤.</p>
            </div>
        </section>

        <!-- ì½”ë“œ ì˜ˆì œ ì„¹ì…˜ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-tabs">
                <button class="code-tab active" data-lang="python">Python</button>
            </div>
            <div class="code-block" data-lang="python">
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
                <pre><code># vLLM ì™„ì „ ê°€ì´ë“œ: ì„œë²„ êµ¬ì¶•, í´ë¼ì´ì–¸íŠ¸, ë²¤ì¹˜ë§ˆí‚¹
# pip install vllm openai

# ============================================
# 1. vLLM ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ì—ì„œ)
# ============================================
# ê¸°ë³¸ ì‹¤í–‰ (ë‹¨ì¼ GPU)
# python -m vllm.entrypoints.openai.api_server \
#     --model meta-llama/Llama-3.1-8B-Instruct \
#     --port 8000

# ë©€í‹° GPU (Tensor Parallelism)
# python -m vllm.entrypoints.openai.api_server \
#     --model meta-llama/Llama-3.1-70B-Instruct \
#     --tensor-parallel-size 4 \
#     --port 8000

# ì–‘ìí™” ëª¨ë¸ ì„œë¹™ (ë©”ëª¨ë¦¬ ì ˆì•½)
# python -m vllm.entrypoints.openai.api_server \
#     --model TheBloke/Llama-2-70B-Chat-AWQ \
#     --quantization awq \
#     --port 8000

# ============================================
# 2. Pythonì—ì„œ vLLM ì§ì ‘ ì‚¬ìš©
# ============================================
from vllm import LLM, SamplingParams

def basic_vllm_inference():
    """vLLMìœ¼ë¡œ ì˜¤í”„ë¼ì¸ ë°°ì¹˜ ì¶”ë¡ """
    # ëª¨ë¸ ë¡œë“œ
    llm = LLM(
        model="meta-llama/Llama-3.1-8B-Instruct",
        tensor_parallel_size=1,  # GPU ìˆ˜
        gpu_memory_utilization=0.9,  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        max_model_len=4096  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    )

    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.95,
        max_tokens=512,
        stop=["</s>", "[/INST]"]
    )

    # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸
    prompts = [
        "[INST] Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. [/INST]",
        "[INST] ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”. [/INST]",
        "[INST] Dockerì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”. [/INST]",
    ]

    # ì¶”ë¡  (ìë™ ë°°ì¹­)
    outputs = llm.generate(prompts, sampling_params)

    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt[:50]}...")
        print(f"Output: {generated_text[:200]}...")
        print("-" * 50)

    return outputs

# ============================================
# 3. OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
# ============================================
from openai import OpenAI

def vllm_openai_client():
    """vLLM ì„œë²„ë¥¼ OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ í˜¸ì¶œ"""
    # vLLM ì„œë²„ ì£¼ì†Œ (ë¡œì»¬)
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="dummy"  # vLLMì€ API í‚¤ ê²€ì¦ ì•ˆ í•¨
    )

    # Chat Completion
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": "vLLMì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
        ],
        temperature=0.7,
        max_tokens=500
    )

    print(response.choices[0].message.content)
    return response

# ============================================
# 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
# ============================================
def vllm_streaming():
    """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í† í° ë‹¨ìœ„ ì‘ë‹µ ë°›ê¸°"""
    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="dummy"
    )

    stream = client.chat.completions.create(
        model="meta-llama/Llama-3.1-8B-Instruct",
        messages=[
            {"role": "user", "content": "Pythonì˜ GILì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
        ],
        stream=True,
        max_tokens=300
    )

    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            print(content, end="", flush=True)
            full_response += content

    return full_response

# ============================================
# 5. ë²¤ì¹˜ë§ˆí‚¹ ë° ì„±ëŠ¥ ì¸¡ì •
# ============================================
import time
import asyncio
import aiohttp

async def benchmark_vllm(
    base_url: str = "http://localhost:8000",
    num_requests: int = 100,
    concurrency: int = 10
):
    """vLLM ì„œë²„ ë²¤ì¹˜ë§ˆí‚¹"""
    prompt = "What is the capital of France? Answer in one word."

    async def single_request(session):
        start = time.time()
        async with session.post(
            f"{base_url}/v1/completions",
            json={
                "model": "meta-llama/Llama-3.1-8B-Instruct",
                "prompt": prompt,
                "max_tokens": 50,
                "temperature": 0
            }
        ) as response:
            result = await response.json()
            latency = time.time() - start
            tokens = result.get("usage", {}).get("completion_tokens", 0)
            return latency, tokens

    # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
    async with aiohttp.ClientSession() as session:
        semaphore = asyncio.Semaphore(concurrency)

        async def bounded_request():
            async with semaphore:
                return await single_request(session)

        start_time = time.time()
        results = await asyncio.gather(*[
            bounded_request() for _ in range(num_requests)
        ])
        total_time = time.time() - start_time

    # ê²°ê³¼ ë¶„ì„
    latencies = [r[0] for r in results]
    total_tokens = sum(r[1] for r in results)

    print(f"=== vLLM Benchmark Results ===")
    print(f"Total requests: {num_requests}")
    print(f"Concurrency: {concurrency}")
    print(f"Total time: {total_time:.2f}s")
    print(f"Throughput: {num_requests / total_time:.2f} req/s")
    print(f"Token throughput: {total_tokens / total_time:.2f} tokens/s")
    print(f"Avg latency: {sum(latencies) / len(latencies) * 1000:.2f} ms")
    print(f"P50 latency: {sorted(latencies)[len(latencies)//2] * 1000:.2f} ms")
    print(f"P99 latency: {sorted(latencies)[int(len(latencies)*0.99)] * 1000:.2f} ms")

# ============================================
# 6. í”„ë¡œë•ì…˜ ì„¤ì • ì˜ˆì‹œ (Docker Compose)
# ============================================
DOCKER_COMPOSE_EXAMPLE = """
# docker-compose.yml
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: >
      --model meta-llama/Llama-3.1-8B-Instruct
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.9
      --max-model-len 8192
      --enable-prefix-caching
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
"""

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜¤í”„ë¼ì¸ ì¶”ë¡  (vLLM ì§ì ‘ ì‚¬ìš©)
    # basic_vllm_inference()

    # ì„œë²„ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
    # vllm_openai_client()

    # ìŠ¤íŠ¸ë¦¬ë°
    # vllm_streaming()

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    # asyncio.run(benchmark_vllm())

    print("vLLM ì½”ë“œ ì˜ˆì œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ì„œë²„ ì‹¤í–‰: python -m vllm.entrypoints.openai.api_server --model <model_name>")</code></pre>
            </div>
        </section>

        <!-- ì„±ëŠ¥ & ë¹„ìš© ì„¹ì…˜ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“Š ì„±ëŠ¥ & ë¹„ìš©</h2>
            <div class="section-content">
                <p><strong>vLLM ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (2025ë…„ ê¸°ì¤€)</strong></p>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>ëª¨ë¸</th>
                            <th>GPU</th>
                            <th>ì²˜ë¦¬ëŸ‰ (tokens/s)</th>
                            <th>P99 ì§€ì—°ì‹œê°„</th>
                            <th>ë¹„ê³ </th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Llama 3.1 8B</td>
                            <td>H100 80GB</td>
                            <td>2,300~2,500</td>
                            <td>80ms</td>
                            <td>ë‹¨ì¼ GPU</td>
                        </tr>
                        <tr>
                            <td>Llama 3.1 70B</td>
                            <td>4x A100 80GB</td>
                            <td>400~600</td>
                            <td>150ms</td>
                            <td>TP=4</td>
                        </tr>
                        <tr>
                            <td>Qwen2.5 7B</td>
                            <td>3x V100 32GB</td>
                            <td>1,782~2,474</td>
                            <td>~100ms</td>
                            <td>TP=2</td>
                        </tr>
                        <tr>
                            <td>Gemma3 4B</td>
                            <td>A100 40GB</td>
                            <td>3,000+</td>
                            <td>50ms</td>
                            <td>ì†Œí˜• ëª¨ë¸</td>
                        </tr>
                        <tr>
                            <td>Mistral 7B AWQ</td>
                            <td>RTX 4090 24GB</td>
                            <td>800~1,200</td>
                            <td>120ms</td>
                            <td>ì–‘ìí™”</td>
                        </tr>
                    </tbody>
                </table>

                <p style="margin-top: 1.5rem;"><strong>vLLM vs ê²½ìŸ ì†”ë£¨ì…˜ ë¹„êµ</strong></p>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>ì†”ë£¨ì…˜</th>
                            <th>í”¼í¬ ì²˜ë¦¬ëŸ‰</th>
                            <th>P99 ì§€ì—°ì‹œê°„</th>
                            <th>ë©”ëª¨ë¦¬ íš¨ìœ¨</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>vLLM</td>
                            <td>793 TPS</td>
                            <td>80ms</td>
                            <td>60~80% ì ˆê°</td>
                        </tr>
                        <tr>
                            <td>Ollama</td>
                            <td>41 TPS</td>
                            <td>673ms</td>
                            <td>ê¸°ë³¸</td>
                        </tr>
                        <tr>
                            <td>HuggingFace TGI</td>
                            <td>~500 TPS</td>
                            <td>~120ms</td>
                            <td>Flash Attention</td>
                        </tr>
                        <tr>
                            <td>TensorRT-LLM</td>
                            <td>~900 TPS</td>
                            <td>~70ms</td>
                            <td>NVIDIA ìµœì í™”</td>
                        </tr>
                    </tbody>
                </table>

                <p style="margin-top: 1.5rem;"><strong>GPU ìš”êµ¬ì‚¬í•­ ê°€ì´ë“œ</strong></p>
                <table class="perf-table">
                    <thead>
                        <tr>
                            <th>ëª¨ë¸ í¬ê¸°</th>
                            <th>FP16 VRAM</th>
                            <th>AWQ/GPTQ</th>
                            <th>ê¶Œì¥ GPU</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>7B</td>
                            <td>~14GB</td>
                            <td>~4GB</td>
                            <td>RTX 4090 / A10</td>
                        </tr>
                        <tr>
                            <td>13B</td>
                            <td>~26GB</td>
                            <td>~7GB</td>
                            <td>A100 40GB</td>
                        </tr>
                        <tr>
                            <td>34B</td>
                            <td>~68GB</td>
                            <td>~18GB</td>
                            <td>A100 80GB</td>
                        </tr>
                        <tr>
                            <td>70B</td>
                            <td>~140GB</td>
                            <td>~35GB</td>
                            <td>2x A100 80GB</td>
                        </tr>
                        <tr>
                            <td>405B</td>
                            <td>~810GB</td>
                            <td>~100GB</td>
                            <td>8x H100 80GB</td>
                        </tr>
                    </tbody>
                </table>

                <div class="perf-note">
                    <strong>ì°¸ê³ :</strong> vLLM 0.6.0 ê¸°ì¤€ ì´ì „ ë²„ì „ ëŒ€ë¹„ 2.7ë°° ì²˜ë¦¬ëŸ‰ í–¥ìƒ, 5ë°° ì§€ì—°ì‹œê°„ ê°ì†Œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. H100ì—ì„œ Llama-3.3-70B FP8 ì¶”ë¡  ì‹œ í† í°ë‹¹ 0.39 joulesë¡œ ì—ë„ˆì§€ íš¨ìœ¨ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. KV ìºì‹œë¥¼ ìœ„í•´ ëª¨ë¸ VRAM ì™¸ ì¶”ê°€ 40GB+ ì—¬ìœ ê°€ í•„ìš”í•©ë‹ˆë‹¤.
                </div>
            </div>
        </section>

        <!-- ì‹¤ë¬´ ëŒ€í™” ì˜ˆì‹œ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ íšŒì˜ì—ì„œ</div>
                    <p class="conv-quote">"LLM API ë¹„ìš©ì´ ë„ˆë¬´ ë†’ì•„ì„œ ìì²´ ì„œë¹™ì„ ê²€í†  ì¤‘ì¸ë°ìš”." - "vLLMìœ¼ë¡œ ì…€í”„í˜¸ìŠ¤íŒ…í•˜ë©´ ë¹„ìš©ì„ 90% ì´ìƒ ì¤„ì¼ ìˆ˜ ìˆì–´ìš”. H100 í•œ ëŒ€ë¡œ Llama 8Bë¥¼ ì„œë¹™í•˜ë©´ ì´ˆë‹¹ 2,000í† í° ì´ìƒ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê³ , OpenAI APIì™€ 100% í˜¸í™˜ë¼ì„œ ì½”ë“œ ë³€ê²½ë„ base_urlë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤. ì´ˆê¸° GPU ë¹„ìš©ì´ ìˆì§€ë§Œ, ì›” 10ë§Œ ìš”ì²­ ì´ìƒì´ë©´ 6ê°œì›” ë‚´ íšŒìˆ˜ë¼ìš”."</p>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <p class="conv-quote">"vLLMì˜ PagedAttentionì´ ì™œ ì¤‘ìš”í•œê°€ìš”?" - "ê¸°ì¡´ LLM ì¶”ë¡ ì—ì„œ KV ìºì‹œëŠ” ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´ë§Œí¼ ë©”ëª¨ë¦¬ë¥¼ ë¯¸ë¦¬ í• ë‹¹í•´ì•¼ í–ˆì–´ìš”. ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” ê±´ ì¼ë¶€ì¸ë° ë‚˜ë¨¸ì§€ëŠ” ë‚­ë¹„ë˜ì£ . PagedAttentionì€ OSì˜ ê°€ìƒ ë©”ëª¨ë¦¬ì²˜ëŸ¼ í•„ìš”í•œ ë§Œí¼ë§Œ ë¸”ë¡ ë‹¨ìœ„ë¡œ í• ë‹¹í•´ì„œ ë©”ëª¨ë¦¬ ë‚­ë¹„ë¥¼ 60~80% ì¤„ì…ë‹ˆë‹¤. ë•ë¶„ì— ê°™ì€ GPUë¡œ ë” ë§ì€ ë™ì‹œ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ìš”."</p>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ê¸°ìˆ  í† ë¡ ì—ì„œ</div>
                    <p class="conv-quote">"vLLM vs TensorRT-LLM ì–´ë–¤ ê±¸ ì¨ì•¼ í•˜ë‚˜ìš”?" - "vLLMì€ ì„¤ì¹˜ê°€ ì‰½ê³  ë‹¤ì–‘í•œ ëª¨ë¸ì„ ë°”ë¡œ ì§€ì›í•´ì„œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì— ì¢‹ì•„ìš”. TensorRT-LLMì€ NVIDIAê°€ ë§Œë“¤ì–´ì„œ H100ì—ì„œ ìµœì  ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ë§Œ, ëª¨ë¸ ë³€í™˜ ê³¼ì •ì´ í•„ìš”í•˜ê³  NVIDIA GPU ì „ìš©ì´ì—ìš”. ì²˜ìŒ ì‹œì‘ì´ê±°ë‚˜ ë‹¤ì–‘í•œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  ë• vLLM, í”„ë¡œë•ì…˜ì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì´ í•„ìš”í•˜ë©´ TensorRT-LLMì„ ê³ ë ¤í•˜ì„¸ìš”."</p>
                </div>
            </div>
        </section>

        <!-- ì£¼ì˜ì‚¬í•­ -->
        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>GPU ë©”ëª¨ë¦¬ ê³¼ë‹¤ í• ë‹¹</strong>
                        <p>gpu_memory_utilizationì„ 0.95 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ë©´ KV ìºì‹œ ê³µê°„ì´ ë¶€ì¡±í•´ OOMì´ ë°œìƒí•©ë‹ˆë‹¤. 0.85~0.90ì´ ì•ˆì „í•˜ë©°, ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë©´ max_model_lenì„ ëª…ì‹œì ìœ¼ë¡œ ì œí•œí•˜ì„¸ìš”.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>Tensor Parallelism í¬ê¸° ë¶ˆì¼ì¹˜</strong>
                        <p>tensor_parallel_sizeëŠ” ëª¨ë¸ì˜ attention head ìˆ˜ë¡œ ë‚˜ëˆ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 32ê°œ head ëª¨ë¸ì— TP=3ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ GPU ê°„ í†µì‹  ëŒ€ì—­í­ì´ ë‚®ìœ¼ë©´ ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆì–´ NVLink ì—°ê²°ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ì ‘ê·¼ ë°©ë²•</strong>
                        <p>í”„ë¡œë•ì…˜ì—ì„œëŠ” --enable-prefix-caching ì˜µì…˜ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìºì‹±ì„ í™œì„±í™”í•˜ì„¸ìš”. Prometheus ë©”íŠ¸ë¦­(/metrics)ì„ ëª¨ë‹ˆí„°ë§í•˜ê³ , HuggingFace ëª¨ë¸ì€ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ(huggingface-cli download)ë¡œ ì½œë“œ ìŠ¤íƒ€íŠ¸ë¥¼ ë°©ì§€í•˜ì„¸ìš”. ì–‘ìí™” ëª¨ë¸(AWQ/GPTQ)ë¡œ ë©”ëª¨ë¦¬ ëŒ€ë¹„ ì²˜ë¦¬ëŸ‰ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- ê´€ë ¨ ìš©ì–´ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/LLM/" class="related-term-link">LLM</a>
                <a href="/ko/term/Inference/" class="related-term-link">Inference</a>
                <a href="/ko/term/Quantization/" class="related-term-link">Quantization</a>
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/Attention/" class="related-term-link">Attention</a>
                <a href="/ko/term/Fine-tuning/" class="related-term-link">Fine-tuning</a>
            </div>
        </section>

        <!-- ë” ë°°ìš°ê¸° -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://docs.vllm.ai/" target="_blank" class="learn-link">
                    <span>ğŸ“„ vLLM ê³µì‹ ë¬¸ì„œ</span>
                </a>
                <a href="https://arxiv.org/abs/2309.06180" target="_blank" class="learn-link">
                    <span>ğŸ“ PagedAttention ë…¼ë¬¸: Efficient Memory Management for LLM Serving</span>
                </a>
                <a href="https://github.com/vllm-project/vllm" target="_blank" class="learn-link">
                    <span>ğŸ“ vLLM GitHub Repository</span>
                </a>
            </div>
        </section>
    </main>

    <!-- Footer -->
        <div id="kaitrust-footer"></div>

    <!-- Scripts -->
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
    <script>
    function copyCode(btn) {
        const codeBlock = btn.parentElement.querySelector('code');
        navigator.clipboard.writeText(codeBlock.textContent).then(() => {
            btn.textContent = 'âœ… ë³µì‚¬ë¨!';
            setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000);
        });
    }
    document.querySelectorAll('.code-tab').forEach(tab => {
        tab.addEventListener('click', function() {
            const lang = this.dataset.lang;
            const section = this.closest('.term-section');
            section.querySelectorAll('.code-tab').forEach(t => t.classList.remove('active'));
            this.classList.add('active');
            section.querySelectorAll('.code-block').forEach(block => {
                block.style.display = block.dataset.lang === lang ? 'block' : 'none';
            });
        });
    });
    </script>
<script src="/glossary/js/term-sections.js?v=20260129231616"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
