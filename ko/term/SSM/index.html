<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SSM (State Space Model) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ìƒíƒœ ê³µê°„ ëª¨ë¸. ì„ í˜• ì‹œìŠ¤í…œ ì´ë¡  ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§. Mambaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜. Transformer ëŒ€ì•ˆ.">
    <meta name="keywords" content="SSM, State Space Model, Mamba, S4, AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/SSM/">

    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/SSM/">
    <meta property="og:title" content="SSM (State Space Model) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ìƒíƒœ ê³µê°„ ëª¨ë¸. ì„ í˜• ì‹œìŠ¤í…œ ì´ë¡  ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§. Mambaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜.">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="SSM (State Space Model) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ìƒíƒœ ê³µê°„ ëª¨ë¸. ì„ í˜• ì‹œìŠ¤í…œ ì´ë¡  ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§. Mambaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜.">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "DefinedTerm",
        "name": "SSM",
        "description": "ìƒíƒœ ê³µê°„ ëª¨ë¸. ì„ í˜• ì‹œìŠ¤í…œ ì´ë¡  ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§. Mambaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜.",
        "inDefinedTermSet": {
            "@type": "DefinedTermSet",
            "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „",
            "url": "https://glossary.kaitrust.ai/"
        }
    }
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container { max-width: 900px; margin: 0 auto; padding: 120px 2rem 4rem; position: relative; z-index: 1; }
        .breadcrumb { display: flex; align-items: center; gap: 0.5rem; margin-bottom: 2rem; font-size: 0.9rem; flex-wrap: wrap; }
        .breadcrumb a { color: #64748b; text-decoration: none; transition: color 0.2s; }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header { background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6)); border: 1px solid rgba(168, 85, 247, 0.2); border-radius: 24px; padding: 3rem; margin-bottom: 2rem; position: relative; overflow: hidden; }
        .term-category-badge { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.2); border-radius: 20px; font-size: 0.85rem; color: #a855f7; margin-bottom: 1rem; }
        .term-title { font-family: 'Orbitron', sans-serif; font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem; background: linear-gradient(135deg, #ffffff, #a855f7); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .term-english { font-size: 1.2rem; color: #94a3b8; margin-bottom: 1.5rem; }
        .term-description { font-size: 1.1rem; line-height: 1.8; color: #e2e8f0; }
        .term-actions { display: flex; gap: 1rem; margin-top: 2rem; flex-wrap: wrap; }
        .term-action-btn { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1.5rem; border-radius: 12px; font-size: 0.9rem; text-decoration: none; transition: all 0.3s; }
        .btn-primary { background: linear-gradient(135deg, #a855f7, #6366f1); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3); }
        .btn-secondary { background: rgba(255, 255, 255, 0.1); color: #e2e8f0; border: 1px solid rgba(255, 255, 255, 0.2); }
        .btn-secondary:hover { background: rgba(255, 255, 255, 0.2); }
        @media (max-width: 768px) { .term-detail-container { padding: 100px 1rem 2rem; } .term-detail-header { padding: 2rem 1.5rem; } .term-title { font-size: 1.8rem; } }
        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .section-content p:last-child { margin-bottom: 0; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; line-height: 1.6; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; font-size: 0.85rem; transition: all 0.2s; }
        .copy-btn:hover { background: rgba(168, 85, 247, 0.5); }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0, 0, 0, 0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; font-size: 0.95rem; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.1); border: 1px solid rgba(168, 85, 247, 0.3); border-radius: 20px; color: #a855f7; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; }
        .related-term-link:hover { background: rgba(168, 85, 247, 0.2); transform: translateY(-2px); }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; transition: all 0.2s; }
        .learn-link:hover { background: rgba(0, 0, 0, 0.3); transform: translateX(5px); }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260129231616">

</head>
<body>
    <div class="particle-container" id="particles"></div>
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a><span>â€º</span>
            <span class="current">SSM</span>
        </nav>

        <article class="term-detail-header">
            <div class="term-category-badge"><span>ğŸ¤–</span><span>AI/ML</span></div>
            <h1 class="term-title">SSM</h1>
            <p class="term-english">State Space Model</p>
            <div class="term-description">
                <p>ìƒíƒœ ê³µê°„ ëª¨ë¸. ì„ í˜• ì‹œìŠ¤í…œ ì´ë¡  ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§. Mambaì˜ í•µì‹¬ ì•„í‚¤í…ì²˜. Transformerì˜ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒ.</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°</a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">ğŸ¤– AI/ML ë”ë³´ê¸°</a>
            </div>
        </article>

        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>SSM(State Space Model, ìƒíƒœ ê³µê°„ ëª¨ë¸)ì€ ì œì–´ ì´ë¡ ê³¼ ì‹ í˜¸ ì²˜ë¦¬ì—ì„œ ìœ ë˜í•œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ê¸°ë²•ì…ë‹ˆë‹¤. ìˆ¨ê²¨ì§„ ìƒíƒœ(hidden state) h(t)ë¥¼ í†µí•´ ì…ë ¥ ì‹œí€€ìŠ¤ u(t)ë¥¼ ì¶œë ¥ y(t)ë¡œ ë§¤í•‘í•˜ë©°, ì—°ì† ì‹œê°„ ë¯¸ë¶„ë°©ì •ì‹ h'(t) = Ah(t) + Bu(t), y(t) = Ch(t) + Du(t)ë¥¼ ì´ì‚°í™”í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. A, B, C, D í–‰ë ¬ì´ ì‹œìŠ¤í…œì˜ ë™ì  íŠ¹ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤.</p>
                <p>2021ë…„ Stanfordì˜ S4(Structured State Spaces for Sequence Modeling) ë…¼ë¬¸ì—ì„œ ë”¥ëŸ¬ë‹ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. HiPPO(High-order Polynomial Projection Operators) ì´ˆê¸°í™”ë¡œ ê¸´ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•©ë‹ˆë‹¤. 2023ë…„ Carnegie Mellonì˜ Mambaê°€ Selective SSMìœ¼ë¡œ Transformerì— í•„ì í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œ í° ì£¼ëª©ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. Transformerì˜ O(n^2) ë³µì¡ë„ë¥¼ O(n)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì— íšê¸°ì ìœ¼ë¡œ ìœ ë¦¬í•©ë‹ˆë‹¤.</p>
                <p>í•µì‹¬ ì¥ì ì€ ì„ í˜• ì‹œê°„ ë³µì¡ë„ì™€ ì´ë¡ ìƒ ë¬´í•œí•œ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. TransformerëŠ” KV ìºì‹œê°€ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€í•´ ì¦ê°€í•˜ì§€ë§Œ, SSMì€ ê³ ì • í¬ê¸° ìƒíƒœ(ë³´í†µ 16-64ì°¨ì›)ë¡œ ê³¼ê±° ì •ë³´ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤. Mambaì˜ Selective SSMì€ ì…ë ¥ì— ë”°ë¼ A, B, C íŒŒë¼ë¯¸í„°ê°€ ë™ì ìœ¼ë¡œ ë³€í•´ Attentionì²˜ëŸ¼ ê´€ë ¨ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ê¸°ì–µí•©ë‹ˆë‹¤. ì¶”ë¡  ì‹œ í† í°ë‹¹ ê³„ì‚°ëŸ‰ì´ ì¼ì •í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ì— ì´ìƒì ì…ë‹ˆë‹¤.</p>
                <p>Mamba 2(2024)ëŠ” í•˜ë“œì›¨ì–´ ì¹œí™”ì  ì„¤ê³„ë¡œ Mamba 1 ëŒ€ë¹„ 2-8ë°° ë¹ ë¥¸ í•™ìŠµì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. Jamba(AI21, Mamba+Attention í•˜ì´ë¸Œë¦¬ë“œ), Zamba(Zyphra), Falcon Mamba ë“± SSM ê¸°ë°˜ ëŒ€ê·œëª¨ ëª¨ë¸ë“¤ì´ ë“±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. DNA/ë‹¨ë°±ì§ˆ ì‹œí€€ìŠ¤(100K+ ê¸¸ì´), ì˜¤ë””ì˜¤(16kHz x ìˆ˜ë¶„), ê¸´ ë¬¸ì„œ ì²˜ë¦¬, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë“±ì—ì„œ ê°•ì ì„ ë³´ì´ë©°, Transformerë¥¼ ë³´ì™„í•˜ê±°ë‚˜ ëŒ€ì²´í•  ì°¨ì„¸ëŒ€ ì•„í‚¤í…ì²˜ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.</p>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python">import torch
import torch.nn as nn
from mamba_ssm import Mamba

# Mamba ë¸”ë¡ ì‚¬ìš© ì˜ˆì‹œ (mamba-ssm ë¼ì´ë¸ŒëŸ¬ë¦¬)
class MambaLM(nn.Module):
    """Mamba ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸"""
    def __init__(
        self,
        vocab_size: int = 50257,
        d_model: int = 768,
        n_layer: int = 12,
        d_state: int = 16,
        expand: int = 2
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Mamba ë ˆì´ì–´ ìŠ¤íƒ
        self.layers = nn.ModuleList([
            Mamba(
                d_model=d_model,
                d_state=d_state,  # SSM ìƒíƒœ ì°¨ì›
                d_conv=4,         # ë¡œì»¬ ì»¨ë³¼ë£¨ì…˜ í¬ê¸°
                expand=expand     # í™•ì¥ ë¹„ìœ¨
            )
            for _ in range(n_layer)
        ])

        self.norm = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        x = self.embedding(input_ids)

        for layer in self.layers:
            x = layer(x) + x  # Residual connection

        x = self.norm(x)
        logits = self.lm_head(x)
        return logits

# ëª¨ë¸ ìƒì„± ë° ì¶”ë¡ 
model = MambaLM(vocab_size=50257, d_model=768, n_layer=12)
input_ids = torch.randint(0, 50257, (1, 2048))  # ë°°ì¹˜ 1, ê¸¸ì´ 2048

with torch.no_grad():
    logits = model(input_ids)
    print(f"ì¶œë ¥ shape: {logits.shape}")  # [1, 2048, 50257]

# Transformer ëŒ€ë¹„ ì¥ì :
# - ì¶”ë¡ : O(n) vs O(n^2) - ê¸´ ì‹œí€€ìŠ¤ì—ì„œ 5-10ë°° ë¹ ë¦„
# - ë©”ëª¨ë¦¬: ê³ ì • ìƒíƒœ í¬ê¸° vs ì¦ê°€í•˜ëŠ” KV ìºì‹œ
# - í•™ìŠµ: FlashAttention ì—†ì´ë„ íš¨ìœ¨ì 

# === Mamba 2 (State Space Duality) ===
# Mamba 2ëŠ” SSMê³¼ Attentionì˜ ì´ë¡ ì  ì—°ê²°ì„±ì„ í™œìš©
# ë” í•˜ë“œì›¨ì–´ ì¹œí™”ì ì¸ ì„¤ê³„ë¡œ 2-8ë°° ë¹ ë¥¸ í•™ìŠµ

# === Jamba: Hybrid SSM + Attention ===
# AI21ì˜ JambaëŠ” Mambaì™€ Attentionì„ êµì°¨ ë°°ì¹˜
# SSMì˜ íš¨ìœ¨ì„± + Attentionì˜ in-context learning ëŠ¥ë ¥

from transformers import AutoModelForCausalLM

# Jamba ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ (52B íŒŒë¼ë¯¸í„°, 140K ì»¨í…ìŠ¤íŠ¸)
jamba_model = AutoModelForCausalLM.from_pretrained(
    "ai21labs/Jamba-v0.1",
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# === ë³µì¡ë„ ë¹„êµ ===
def complexity_comparison(seq_len):
    """Transformer vs SSM ë³µì¡ë„ ë¹„êµ"""
    transformer_attention = seq_len ** 2  # O(n^2)
    transformer_kv_cache = seq_len * 2 * 768  # 2 * d_model per layer
    ssm_compute = seq_len  # O(n)
    ssm_state = 64  # ê³ ì • ìƒíƒœ í¬ê¸° (d_state)

    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len:,}")
    print(f"Transformer Attention ì—°ì‚°: {transformer_attention:,}")
    print(f"Transformer KV Cache (1 layer): {transformer_kv_cache:,} floats")
    print(f"SSM ì—°ì‚° (í† í°ë‹¹): {ssm_compute:,}")
    print(f"SSM ìƒíƒœ í¬ê¸°: {ssm_state} (ê³ ì •)")
    print(f"íš¨ìœ¨ì„± ë¹„ìœ¨: {transformer_attention / seq_len:.0f}x")

complexity_comparison(100000)  # 100K í† í°

# === HiPPO ì´ˆê¸°í™” (ê¸´ ì˜ì¡´ì„± í•™ìŠµ) ===
# HiPPO(High-order Polynomial Projection Operators)ëŠ”
# ê¸´ ì‹œí€€ìŠ¤ ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” A í–‰ë ¬ ì´ˆê¸°í™”

# === SSM vs RNN vs Transformer ë¹„êµ ===
# |           | í•™ìŠµ ë³µì¡ë„ | ì¶”ë¡  ë³µì¡ë„ | ë©”ëª¨ë¦¬ | ê¸´ ì˜ì¡´ì„± |
# |-----------|------------|------------|--------|----------|
# | RNN       | O(n)       | O(1)/í† í°  | O(1)   | ì–´ë ¤ì›€   |
# | Transformer| O(n^2)    | O(n)/í† í°  | O(n)   | ì¢‹ìŒ     |
# | SSM       | O(n)       | O(1)/í† í°  | O(1)   | ì¢‹ìŒ     |

# SSMì€ RNNì˜ íš¨ìœ¨ì„± + Transformerì˜ ì„±ëŠ¥ì„ ê²°í•©!

# === ì‹¤ì œ ì‘ìš© ë¶„ì•¼ ===
# 1. DNA/ë‹¨ë°±ì§ˆ ì‹œí€€ìŠ¤ (ìˆ˜ì‹­ë§Œ base pair)
# 2. ì˜¤ë””ì˜¤ ì²˜ë¦¬ (16kHz x ìˆ˜ ë¶„ = ìˆ˜ë°±ë§Œ ìƒ˜í”Œ)
# 3. ì´ˆì¥ë¬¸ ë¬¸ì„œ (ë²•ë¥ , ì˜ë£Œ, ì½”ë“œë² ì´ìŠ¤)
# 4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (ì¼ì •í•œ ì§€ì—°ì‹œê°„ ë³´ì¥)

# === Mamba ìŠ¤íƒ€ì¼ Selective SSM í•µì‹¬ ===
# ê¸°ì¡´ SSM: A, B, Cê°€ ì…ë ¥ê³¼ ë¬´ê´€í•œ ê³ ì •ê°’
# Selective SSM: A, B, Cê°€ ì…ë ¥ xì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²°ì •

# x -> Linear -> B, C, delta ìƒì„±
# deltaë¡œ Aë¥¼ ì´ì‚°í™” (ë‹¤ë¥¸ ì‹œê°„ í•´ìƒë„ ì ìš©)
# ì´ë¥¼ í†µí•´ ì…ë ¥ì— ë”°ë¼ ë¬´ì—‡ì„ ê¸°ì–µ/ë§ê°í• ì§€ ì„ íƒ

# === State Space Duality (Mamba 2) ===
# SSMê³¼ Attentionì´ ìˆ˜í•™ì ìœ¼ë¡œ ì—°ê²°ëœë‹¤ëŠ” ë°œê²¬
# íŠ¹ì • ì¡°ê±´ì—ì„œ SSMì˜ ì¶œë ¥ = êµ¬ì¡°í™”ëœ Attentionê³¼ ë™ì¼
# ì´ë¥¼ í™œìš©í•´ ë” í•˜ë“œì›¨ì–´ ì¹œí™”ì ì¸ êµ¬í˜„ ê°€ëŠ¥

# === Zamba (Zyphra) ===
# Mamba + Shared Attention í•˜ì´ë¸Œë¦¬ë“œ
# ì¼ë¶€ ë ˆì´ì–´ë§Œ Attentionì„ ê³µìœ í•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
# 7B ê·œëª¨ì—ì„œ Llama 2 13B ìˆ˜ì¤€ ì„±ëŠ¥

# === Falcon Mamba (Technology Innovation Institute) ===
# ìˆœìˆ˜ Mamba ì•„í‚¤í…ì²˜ë¡œ 7B ëª¨ë¸
# Transformer ì—†ì´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥ ë‹¬ì„±

# === ì£¼ìš” SSM ëª¨ë¸ ë¹„êµ ===
# | ëª¨ë¸      | í¬ê¸°  | ì»¨í…ìŠ¤íŠ¸ | íŠ¹ì§•               |
# |----------|-------|---------|-------------------|
# | Mamba    | 3B    | ë¬´ì œí•œ   | ìˆœìˆ˜ SSM          |
# | Mamba 2  | 2.8B  | ë¬´ì œí•œ   | í•˜ë“œì›¨ì–´ ìµœì í™”    |
# | Jamba    | 52B   | 256K    | SSM + Attention   |
# | Zamba    | 7B    | ë¬´ì œí•œ   | Shared Attention  |
# | RecurrentGemma | 9B | ë¬´ì œí•œ | RG-LRU ê¸°ë°˜       |

# === SSM í•™ìŠµ íŒ ===
# 1. ì´ˆê¸°í™”: HiPPO ì´ˆê¸°í™”ê°€ ê¸´ ì˜ì¡´ì„± í•™ìŠµì— í•„ìˆ˜
# 2. ì •ê·œí™”: LayerNormë³´ë‹¤ RMSNormì´ ë” íš¨ê³¼ì 
# 3. ì”ì°¨ ì—°ê²°: ëª¨ë“  ë¸”ë¡ì— residual connection ì ìš©
# 4. í˜¼í•©: Mamba + MLP ë˜ëŠ” Mamba + Attention í˜¼í•© ê³ ë ¤

# === ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ (Mamba 3B vs Transformer 3B) ===
# | íƒœìŠ¤í¬ | Mamba 3B | Transformer 3B | ë¹„ê³  |
# |-------|----------|----------------|-----|
# | HellaSwag | 74.2 | 73.8 | ë™ë“± |
# | WinoGrande | 68.5 | 68.1 | ë™ë“± |
# | ì¶”ë¡  ì†ë„ | 5x faster | baseline | Mamba ìš°ìœ„ |</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ì•„í‚¤í…ì²˜ ì„ íƒ íšŒì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"100K í† í° ì´ìƒ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ Mambaë‚˜ Jamba ê²€í† í•´ë³´ì„¸ìš”. TransformerëŠ” KV ìºì‹œê°€ 100Kë©´ ìˆ˜ì‹­ GB ë©”ëª¨ë¦¬ê°€ í•„ìš”í•œë°, SSMì€ ìƒíƒœ í¬ê¸°ê°€ ê³ ì •ì´ë¼ 1GB ë¯¸ë§Œìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"SSMì€ ìƒíƒœ ê³µê°„ ëª¨ë¸ë¡œ, x'=Ax+Bu, y=Cx ë¯¸ë¶„ë°©ì •ì‹ì„ ì´ì‚°í™”í•´ì„œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. MambaëŠ” Selective SSMìœ¼ë¡œ ì…ë ¥ì— ë”°ë¼ A,B,C íŒŒë¼ë¯¸í„°ê°€ ë™ì ìœ¼ë¡œ ë°”ë€Œì–´ Attentionì²˜ëŸ¼ ì»¨í…ìŠ¤íŠ¸ ì¸ì§€ê°€ ê°€ëŠ¥í•´ì¡Œì–´ìš”. ë³µì¡ë„ëŠ” O(n)ì…ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ì„±ëŠ¥ ìµœì í™” ë…¼ì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶”ë¡ ì´ë¼ë©´ SSMì´ í™•ì‹¤íˆ ìœ ë¦¬í•´ìš”. TransformerëŠ” í† í°ë§ˆë‹¤ ì „ì²´ KV ìºì‹œë¥¼ ì°¸ì¡°í•˜ëŠ”ë°, MambaëŠ” ì‘ì€ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ë¼ì„œ í† í°ë‹¹ latencyê°€ ì¼ì •í•©ë‹ˆë‹¤."</blockquote>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ëª¨ë“  íƒœìŠ¤í¬ì— SSMì´ ì¢‹ë‹¤ê³  ê°€ì •</strong>
                        <p>In-context learning, ë³µì¡í•œ reasoningì—ì„œëŠ” ì•„ì§ Transformerê°€ ìš°ì„¸í•©ë‹ˆë‹¤. SSMì€ ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë°ì— ê°•ì ì´ ìˆìŠµë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ë¬´ì‹œ</strong>
                        <p>MambaëŠ” CUDA ì»¤ë„ì— ì˜ì¡´í•˜ì—¬ íŠ¹ì • GPU(Ampere+)ì™€ CUDA ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ì„ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ë°©ë²•</strong>
                        <p>Hybrid ëª¨ë¸(Jambaì²˜ëŸ¼ Transformer + Mamba)ì„ ê³ ë ¤í•˜ê±°ë‚˜, ìœ ìŠ¤ì¼€ì´ìŠ¤ì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”. DNA, ì˜¤ë””ì˜¤, ê¸´ ë¬¸ì„œì—ì„œ SSMì´ ê°•í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Mamba/" class="related-term-link">Mamba</a>
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/Attention/" class="related-term-link">Attention</a>
                <a href="/ko/term/RNN/" class="related-term-link">RNN</a>
                <a href="/ko/term/LSTM/" class="related-term-link">LSTM</a>
                <a href="/ko/term/Linear%20Attention/" class="related-term-link">Linear Attention</a>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/2312.00752" target="_blank" class="learn-link">ğŸ“ Mamba: Linear-Time Sequence Modeling</a>
                <a href="https://arxiv.org/abs/2111.00396" target="_blank" class="learn-link">ğŸ“ S4: Structured State Spaces</a>
                <a href="https://github.com/state-spaces/mamba" target="_blank" class="learn-link">ğŸ“¦ Mamba GitHub Repository</a>
            </div>
        </section>
    </main>

        <div id="kaitrust-footer"></div>

    <script>
    function copyCode(btn) {
        const codeBlock = btn.parentElement.querySelector('code');
        navigator.clipboard.writeText(codeBlock.textContent).then(() => {
            btn.textContent = 'âœ… ë³µì‚¬ë¨!';
            setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000);
        });
    }
    </script>
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
<script src="/glossary/js/term-sections.js?v=20260129231616"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
