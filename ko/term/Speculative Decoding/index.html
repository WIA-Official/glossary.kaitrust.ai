<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speculative Decoding (ì¶”ì¸¡ì  ë””ì½”ë”©) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ì‘ì€ ëª¨ë¸ë¡œ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  í° ëª¨ë¸ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ë¡  ê°€ì† ê¸°ë²•. ì¶œë ¥ í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ 2-3ë°° ì†ë„ í–¥ìƒ.">
    <meta name="keywords" content="Speculative Decoding, ì¶”ì¸¡ì  ë””ì½”ë”©, LLM ì¶”ë¡  ìµœì í™”, Draft Model, AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/Speculative%20Decoding/">

    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/Speculative%20Decoding/">
    <meta property="og:title" content="Speculative Decoding (ì¶”ì¸¡ì  ë””ì½”ë”©) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ì‘ì€ ëª¨ë¸ë¡œ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  í° ëª¨ë¸ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ë¡  ê°€ì† ê¸°ë²•. ì¶œë ¥ í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ 2-3ë°° ì†ë„ í–¥ìƒ.">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Speculative Decoding (ì¶”ì¸¡ì  ë””ì½”ë”©) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ì‘ì€ ëª¨ë¸ë¡œ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  í° ëª¨ë¸ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ë¡  ê°€ì† ê¸°ë²•. ì¶œë ¥ í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ 2-3ë°° ì†ë„ í–¥ìƒ.">

    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "DefinedTerm", "name": "Speculative Decoding", "description": "ì‘ì€ ëª¨ë¸ë¡œ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  í° ëª¨ë¸ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ë¡  ê°€ì† ê¸°ë²•. ì¶œë ¥ í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ 2-3ë°° ì†ë„ í–¥ìƒ.", "inDefinedTermSet": {"@type": "DefinedTermSet", "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „", "url": "https://glossary.kaitrust.ai/"}}
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container { max-width: 900px; margin: 0 auto; padding: 120px 2rem 4rem; position: relative; z-index: 1; }
        .breadcrumb { display: flex; align-items: center; gap: 0.5rem; margin-bottom: 2rem; font-size: 0.9rem; flex-wrap: wrap; }
        .breadcrumb a { color: #64748b; text-decoration: none; } .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; } .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header { background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6)); border: 1px solid rgba(168, 85, 247, 0.2); border-radius: 24px; padding: 3rem; margin-bottom: 2rem; }
        .term-category-badge { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.2); border-radius: 20px; font-size: 0.85rem; color: #a855f7; margin-bottom: 1rem; }
        .term-title { font-family: 'Orbitron', sans-serif; font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem; background: linear-gradient(135deg, #ffffff, #a855f7); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        .term-english { font-size: 1.2rem; color: #94a3b8; margin-bottom: 1.5rem; }
        .term-description { font-size: 1.1rem; line-height: 1.8; color: #e2e8f0; }
        .term-actions { display: flex; gap: 1rem; margin-top: 2rem; flex-wrap: wrap; }
        .term-action-btn { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1.5rem; border-radius: 12px; font-size: 0.9rem; text-decoration: none; transition: all 0.3s; }
        .btn-primary { background: linear-gradient(135deg, #a855f7, #6366f1); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3); }
        .btn-secondary { background: rgba(255, 255, 255, 0.1); color: #e2e8f0; border: 1px solid rgba(255, 255, 255, 0.2); }
        @media (max-width: 768px) { .term-detail-container { padding: 100px 1rem 2rem; } .term-detail-header { padding: 2rem 1.5rem; } .term-title { font-size: 1.8rem; } }
        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; line-height: 1.6; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; font-size: 0.85rem; }
        .copy-btn:hover { background: rgba(168, 85, 247, 0.5); }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0, 0, 0, 0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; font-size: 0.95rem; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.1); border: 1px solid rgba(168, 85, 247, 0.3); border-radius: 20px; color: #a855f7; text-decoration: none; font-size: 0.9rem; }
        .related-term-link:hover { background: rgba(168, 85, 247, 0.2); transform: translateY(-2px); }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; }
        .learn-link:hover { background: rgba(0, 0, 0, 0.3); transform: translateX(5px); }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260129231616">

</head>
<body>
    <div class="particle-container" id="particles"></div>
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a><span>â€º</span><a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a><span>â€º</span><a href="https://glossary.kaitrust.ai/#ai">AI/ML</a><span>â€º</span><span class="current">Speculative Decoding</span>
        </nav>

        <article class="term-detail-header">
            <div class="term-category-badge"><span>ğŸ¤–</span><span>AI/ML</span></div>
            <h1 class="term-title">Speculative Decoding</h1>
            <p class="term-english">ì¶”ì¸¡ì  ë””ì½”ë”©</p>
            <div class="term-description">
                <p>ì‘ì€ ëª¨ë¸ë¡œ ì´ˆì•ˆì„ ìƒì„±í•˜ê³  í° ëª¨ë¸ë¡œ ê²€ì¦í•˜ëŠ” ì¶”ë¡  ê°€ì† ê¸°ë²•. ì¶œë ¥ í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ 2-3ë°° ì†ë„ í–¥ìƒ.</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°</a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">ğŸ¤– AI/ML ë”ë³´ê¸°</a>
            </div>
        </article>

        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>Speculative Decodingì€ ì‘ê³  ë¹ ë¥¸ "draft model"ì´ ì—¬ëŸ¬ í† í°ì„ ë¯¸ë¦¬ ìƒì„±í•˜ê³ , í¬ê³  ì •í™•í•œ "target model"ì´ ì´ë¥¼ í•œ ë²ˆì— ê²€ì¦í•˜ëŠ” LLM ì¶”ë¡  ê°€ì† ê¸°ë²•ì…ë‹ˆë‹¤. CPUì˜ ë¶„ê¸° ì˜ˆì¸¡(branch prediction)ê³¼ ìœ ì‚¬í•œ ì•„ì´ë””ì–´ë¡œ, ì˜ˆì¸¡ì´ ë§ìœ¼ë©´ í° ì´ë“ì„ ì–»ê³ , í‹€ë ¤ë„ ì†í•´ê°€ ì‘ìŠµë‹ˆë‹¤. í•µì‹¬ì€ draftê°€ í‹€ë ¤ë„ target modelì˜ ì¶œë ¥ ë¶„í¬ê°€ 100% ë³´ì¡´ëœë‹¤ëŠ” ìˆ˜í•™ì  ë³´ì¥ì…ë‹ˆë‹¤.</p>
                <p>2022ë…„ DeepMindì˜ "Fast Inference from Transformers via Speculative Decoding" ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ autoregressive ìƒì„±ì€ í† í° í•˜ë‚˜ë§ˆë‹¤ ê±°ëŒ€ ëª¨ë¸ì˜ forward passê°€ í•„ìš”í•´ ëŠë ¸ìŠµë‹ˆë‹¤. 70B ëª¨ë¸ì€ ì´ˆë‹¹ 10-20 í† í°ë°–ì— ìƒì„±í•˜ì§€ ëª»í•©ë‹ˆë‹¤. Speculative Decodingì€ ì´ ë³‘ëª©ì„ ì˜ë¦¬í•œ ë³‘ë ¬í™”ë¡œ í•´ê²°í•©ë‹ˆë‹¤. Googleì˜ Gemini, Anthropicì˜ Claude ë“± ìƒìš© ëª¨ë¸ì—ì„œë„ ë‚´ë¶€ì ìœ¼ë¡œ ìœ ì‚¬í•œ ìµœì í™”ê°€ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
                <p>ë™ì‘ ì›ë¦¬: 1) Draft modelì´ kê°œ í† í° ìƒì„± (ì˜ˆ: "The quick brown fox") 2) Target modelì´ k+1ê°œ ìœ„ì¹˜ì˜ ë¡œì§“ì„ í•œ ë²ˆì— ê³„ì‚° (ë³‘ë ¬ forward) 3) ê° ìœ„ì¹˜ì—ì„œ draft í† í°ì„ acceptance samplingìœ¼ë¡œ ê²€ì¦ 4) ë¶ˆì¼ì¹˜ ì‹œì ì—ì„œ target ë¶„í¬ë¡œ êµì •ëœ í† í° ìƒì„±. ì´ ê³¼ì •ì—ì„œ acceptance probabilityì— ë”°ë¼ í™•ë¥ ì ìœ¼ë¡œ í† í°ì„ accept/rejectí•˜ì—¬ target model ë‹¨ë… ìƒì„±ê³¼ ë™ì¼í•œ ë¶„í¬ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.</p>
                <p>vLLM, TensorRT-LLM, Hugging Face TGI, llama.cpp ë“± ì£¼ìš” ì¶”ë¡  ì—”ì§„ì´ ì§€ì›í•©ë‹ˆë‹¤. Draft modelì€ targetì˜ 1/10~1/5 í¬ê¸°ê°€ ì ë‹¹í•˜ë©°(ì˜ˆ: Llama 70B + Llama 8B), ë°˜ë“œì‹œ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤. ì½”ë“œ ìƒì„±, JSON ì¶œë ¥, ë²ˆì—­ ë“± ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì—ì„œ acceptance rateê°€ 70-90%ì— ë‹¬í•´ 3-4ë°° ì†ë„ í–¥ìƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. Self-Speculative Decoding, Medusa(multiple heads), EAGLE ë“± ë³€í˜• ê¸°ë²•ë„ í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python"># Speculative Decoding ê°œë… êµ¬í˜„ (ê°„ì†Œí™”)
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def speculative_decode(prompt, target_model, draft_model, tokenizer,
                       k=5, max_tokens=100):
    """
    k: draft modelì´ í•œ ë²ˆì— ìƒì„±í•  í† í° ìˆ˜
    """
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    generated = input_ids.clone()

    for _ in range(max_tokens // k):
        # 1. Draft modelë¡œ kê°œ í† í° ë¹ ë¥´ê²Œ ìƒì„±
        draft_ids = generated.clone()
        for _ in range(k):
            with torch.no_grad():
                logits = draft_model(draft_ids).logits[:, -1, :]
                next_token = logits.argmax(dim=-1, keepdim=True)
                draft_ids = torch.cat([draft_ids, next_token], dim=-1)

        draft_tokens = draft_ids[:, generated.shape[1]:]  # ìƒˆë¡œ ìƒì„±ëœ kê°œ

        # 2. Target modelë¡œ í•œ ë²ˆì— ê²€ì¦ (ë³‘ë ¬!)
        with torch.no_grad():
            # draft_ids ì „ì²´ë¥¼ í•œ ë²ˆì— forward
            target_logits = target_model(draft_ids).logits

        # 3. ì¼ì¹˜í•˜ëŠ” í† í°ê¹Œì§€ë§Œ accept
        accepted = 0
        for i in range(k):
            target_pred = target_logits[:, generated.shape[1] + i - 1, :].argmax()
            draft_pred = draft_tokens[:, i]
            if target_pred == draft_pred:
                accepted += 1
            else:
                # ë¶ˆì¼ì¹˜: targetì˜ ì˜ˆì¸¡ìœ¼ë¡œ êµì²´
                generated = torch.cat([generated, target_pred.unsqueeze(0).unsqueeze(0)], dim=-1)
                break
        else:
            # ëª¨ë‘ ì¼ì¹˜: kê°œ ì „ë¶€ accept
            generated = draft_ids

        if tokenizer.eos_token_id in generated[0]:
            break

    return tokenizer.decode(generated[0])

# vLLMì—ì„œ Speculative Decoding ì‚¬ìš© (ì‹¤ì œ í”„ë¡œë•ì…˜)
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3-70b-instruct",
    speculative_model="meta-llama/Llama-3-8b-instruct",  # Draft model
    num_speculative_tokens=5,  # í•œ ë²ˆì— ì¶”ì¸¡í•  í† í° ìˆ˜
    tensor_parallel_size=4
)

sampling_params = SamplingParams(temperature=0.0, max_tokens=500)
output = llm.generate("Explain quantum computing:", sampling_params)
# ì¼ë°˜ ë””ì½”ë”© ëŒ€ë¹„ 2-3x ë¹ ë¦„!

# === Medusa (Multi-Head Speculative Decoding) ===
# ë³„ë„ draft model ì—†ì´ target modelì— ì¶”ê°€ headë¥¼ ë¶™ì„
from transformers import AutoModelForCausalLM
import torch.nn as nn

class MedusaHead(nn.Module):
    """Medusa: ê° headê°€ ë‹¤ìŒ në²ˆì§¸ í† í°ì„ ì˜ˆì¸¡"""
    def __init__(self, hidden_size, vocab_size, num_heads=4):
        super().__init__()
        self.heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.SiLU(),
                nn.Linear(hidden_size, vocab_size)
            )
            for _ in range(num_heads)
        ])

    def forward(self, hidden_states):
        # ê° headê°€ +1, +2, +3, +4 ìœ„ì¹˜ í† í° ì˜ˆì¸¡
        return [head(hidden_states) for head in self.heads]

# Medusa ì¥ì : ë³„ë„ draft model ë¡œë“œ ë¶ˆí•„ìš”, ë©”ëª¨ë¦¬ ì ˆì•½
# ë‹¨ì : ì¶”ê°€ head í•™ìŠµ í•„ìš”

# === ì„±ëŠ¥ ì¸¡ì • ì˜ˆì‹œ ===
import time

def benchmark_generation(model, prompt, max_tokens=100, runs=5):
    """í† í° ìƒì„± ì†ë„ ì¸¡ì •"""
    times = []
    for _ in range(runs):
        start = time.perf_counter()
        output = model.generate(prompt, max_tokens=max_tokens)
        elapsed = time.perf_counter() - start
        times.append(elapsed)

    tokens_per_sec = max_tokens / (sum(times) / len(times))
    return tokens_per_sec

# ì¼ë°˜ ë””ì½”ë”© vs Speculative Decoding ë¹„êµ
# ì¼ë°˜: ~15 tokens/sec (70B model)
# Speculative: ~40-50 tokens/sec (with 8B draft)

# === Draft ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ ===
# Target ëª¨ë¸    | ì¶”ì²œ Draft ëª¨ë¸        | Acceptance Rate
# Llama 70B      | Llama 8B               | 70-85%
# Llama 405B     | Llama 70B ë˜ëŠ” 8B      | 60-80%
# Mixtral 8x22B  | Mistral 7B             | 65-80%
# GPT-4          | GPT-3.5 Turbo (ë‚´ë¶€)   | 75-90%

# === EAGLE (Lossless Acceleration) ===
# EAGLEëŠ” ë³„ë„ draft ëª¨ë¸ ëŒ€ì‹  target ëª¨ë¸ì˜ hidden statesë¥¼ ì¬í™œìš©
# ì¥ì : ì¶”ê°€ ëª¨ë¸ ë¡œë“œ ë¶ˆí•„ìš”, ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
# ë‹¨ì : ì¶”ê°€ head í•™ìŠµ í•„ìš”

# === Self-Speculative Decoding ===
# ë™ì¼ ëª¨ë¸ì˜ ì´ˆê¸° ë ˆì´ì–´ë§Œ ì‚¬ìš©í•´ì„œ draft ìƒì„±
# ì‘ì€ ì„œë¸Œë„·ì´ ë¹ ë¥´ê²Œ ì˜ˆì¸¡, ì „ì²´ ëª¨ë¸ì´ ê²€ì¦

# === ì–¸ì œ Speculative Decodingì´ íš¨ê³¼ì ì¸ê°€? ===
# 1. ì½”ë“œ ìƒì„± (ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¬¸ë²• êµ¬ì¡°)
# 2. JSON/XML ì¶œë ¥ (ì •í˜•í™”ëœ í¬ë§·)
# 3. ë²ˆì—­ (ì…ë ¥ê³¼ ì¶œë ¥ êµ¬ì¡° ìœ ì‚¬)
# 4. ìˆ˜í•™ ê³„ì‚° (ê²°ì •ë¡ ì  íŒ¨í„´)

# ë¹„íš¨ìœ¨ì ì¸ ê²½ìš°:
# - ì°½ì˜ì  ê¸€ì“°ê¸° (ì˜ˆì¸¡ ì–´ë ¤ì›€)
# - ë†’ì€ temperature ìƒ˜í”Œë§
# - ì•„ì£¼ ì§§ì€ ì¶œë ¥

# === Lookahead Decoding (ë³‘ë ¬ ê²€ì¦ ë³€í˜•) ===
# ì—¬ëŸ¬ í›„ë³´ ê²½ë¡œë¥¼ ë™ì‹œì— íƒìƒ‰
# Tree-based speculationìœ¼ë¡œ ë” ë†’ì€ acceptance rate

# === TensorRT-LLMì—ì„œ ì‚¬ìš© ===
# NVIDIA TensorRT-LLMë„ Speculative Decoding ì§€ì›
# configì—ì„œ speculative_decoding í™œì„±í™”

# === Acceptance Rate ëª¨ë‹ˆí„°ë§ ===
def monitor_acceptance_rate(draft_ids, target_ids, window=100):
    """ì‹¤ì‹œê°„ acceptance rate ì¶”ì """
    accepted = sum(d == t for d, t in zip(draft_ids, target_ids))
    rate = accepted / len(draft_ids)
    print(f"Acceptance Rate: {rate:.1%}")
    # rate < 50%ë©´ draft ëª¨ë¸ ë³€ê²½ ê³ ë ¤
    return rate

# === ë¹„ìš© ì ˆê° ê³„ì‚° ===
# Target model: 70B, 15 tokens/sec, $10/1M tokens
# Draft model: 8B, 100 tokens/sec, $1/1M tokens
# Acceptance rate: 75%
# í‰ê·  ì¶”ì¸¡ í† í°: 5ê°œ ì¤‘ 3.75ê°œ ìˆ˜ë½
# ì†ë„ í–¥ìƒ: (1 + 3.75) / (1 + 1) = 2.4ë°°
# ë¹„ìš©: target í˜¸ì¶œ íšŸìˆ˜ 60% ê°ì†Œ

# === ì£¼ìš” ì¶”ë¡  ì—”ì§„ ì§€ì› í˜„í™© ===
# | ì—”ì§„          | ì§€ì› | ë¹„ê³                       |
# |--------------|-----|--------------------------|
# | vLLM         | O   | speculative_model íŒŒë¼ë¯¸í„° |
# | TensorRT-LLM | O   | config ì„¤ì •               |
# | llama.cpp    | O   | --speculative í”Œë˜ê·¸      |
# | HF TGI       | O   | speculation_config        |
# | Ollama       | X   | ë¯¸ì§€ì› (2024.12 ê¸°ì¤€)      |

# === ì‹¤ì „ íŠœë‹ íŒ ===
# 1. num_speculative_tokens: 5ê°€ ê¸°ë³¸, 8ê¹Œì§€ ì‹¤í—˜
# 2. ì •í˜• ì¶œë ¥(JSON)ì—ì„œ ë” ê³µê²©ì ìœ¼ë¡œ (10+)
# 3. ì°½ì˜ì  ê¸€ì“°ê¸°ëŠ” 3 ì´í•˜ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ
# 4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ acceptance rate ì¶”ì 

# === Batch Speculative Decoding ===
# ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•  ë•Œë„ ì ìš© ê°€ëŠ¥
# continuous batchingê³¼ ì¡°í•©í•˜ì—¬ throughput ê·¹ëŒ€í™”

# === Prompt Lookup Decoding ===
# ì…ë ¥ í”„ë¡¬í”„íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ draftë¡œ í™œìš©
# ì½”ë“œ ìˆ˜ì •, ë¬¸ì„œ í¸ì§‘ ë“±ì—ì„œ íš¨ê³¼ì 
# ë³„ë„ draft ëª¨ë¸ ì—†ì´ ì…ë ¥ ìì²´ê°€ íŒíŠ¸ ì—­í• 

# === DistillSpec ===
# Target ëª¨ë¸ì„ teacherë¡œ draft ëª¨ë¸ ì¦ë¥˜(distillation)
# íƒœìŠ¤í¬ íŠ¹í™”ëœ draft ëª¨ë¸ë¡œ acceptance rate í–¥ìƒ</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ LLM ì„œë¹™ ìµœì í™” íšŒì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"Speculative Decoding ì ìš©í•˜ë©´ Llama 70B ì¶”ë¡  ì†ë„ê°€ 2ë°° ë¹¨ë¼ì ¸ìš”. 8B ëª¨ë¸ì„ draftë¡œ ì“°ë©´ ë˜ëŠ”ë°, ê°™ì€ í† í¬ë‚˜ì´ì € ì“°ëŠ” ê²Œ ì¤‘ìš”í•©ë‹ˆë‹¤. vLLMì—ì„œëŠ” speculative_model íŒŒë¼ë¯¸í„°ë§Œ ì¶”ê°€í•˜ë©´ ë¼ìš”."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"Speculative Decodingì€ ì‘ì€ draft modelì´ kê°œ í† í°ì„ ì¶”ì¸¡í•˜ê³ , í° target modelì´ ë³‘ë ¬ë¡œ ê²€ì¦í•©ë‹ˆë‹¤. í•µì‹¬ì€ targetì˜ ì¶œë ¥ ë¶„í¬ê°€ 100% ìœ ì§€ëœë‹¤ëŠ” ì ì´ì—ìš”. Draftê°€ í‹€ë ¤ë„ targetì´ êµì •í•˜ë‹ˆê¹Œ í’ˆì§ˆ ì €í•˜ ì—†ì´ ì†ë„ë§Œ í–¥ìƒë©ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë¹„ìš© ì ˆê° ë…¼ì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"ì½”ë“œ ìƒì„±ì´ë‚˜ JSON ì¶œë ¥ ê°™ì€ ì •í˜•í™”ëœ íƒœìŠ¤í¬ì—ì„œ Speculative Decodingì´ íŠ¹íˆ íš¨ê³¼ì ì´ì—ìš”. ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì´ ë§ì•„ì„œ acceptance rateê°€ 70-80% ë‚˜ì˜¤ê³ , ì¶”ë¡  ë¹„ìš©ì´ ê±°ì˜ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ìš”."</blockquote>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>Draftì™€ Targetì˜ í† í¬ë‚˜ì´ì € ë¶ˆì¼ì¹˜</strong>
                        <p>ì„œë¡œ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì“°ë©´ í† í° ê²½ê³„ê°€ ë§ì§€ ì•Šì•„ ê²€ì¦ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ ê°™ì€ í† í¬ë‚˜ì´ì €(ê°™ì€ ëª¨ë¸ íŒ¨ë°€ë¦¬)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ë„ˆë¬´ í° kê°’ ì„¤ì •</strong>
                        <p>k(ì¶”ì¸¡ í† í° ìˆ˜)ê°€ ë„ˆë¬´ í¬ë©´ rejectionì´ ë§ì•„ì ¸ ì˜¤íˆë ¤ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´í†µ 4-8ì´ ìµœì ì´ë©°, íƒœìŠ¤í¬ë³„ë¡œ íŠœë‹ì´ í•„ìš”í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ë°©ë²•</strong>
                        <p>ê°™ì€ íŒ¨ë°€ë¦¬ ëª¨ë¸(ì˜ˆ: Llama 70B + Llama 8B)ì„ ì‚¬ìš©í•˜ê³ , k=5ë¶€í„° ì‹œì‘í•´ acceptance rateë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”. vLLM, TGI ë“± ê²€ì¦ëœ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Autoregressive/" class="related-term-link">Autoregressive</a>
                <a href="/ko/term/KV%20Cache/" class="related-term-link">KV Cache</a>
                <a href="/ko/term/vLLM/" class="related-term-link">vLLM</a>
                <a href="/ko/term/Quantization/" class="related-term-link">Quantization</a>
                <a href="/ko/term/Inference/" class="related-term-link">Inference</a>
                <a href="/ko/term/Batch%20Processing/" class="related-term-link">Batch Processing</a>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/2211.17192" target="_blank" class="learn-link">ğŸ“ ë…¼ë¬¸: Fast Inference from Transformers via Speculative Decoding</a>
                <a href="https://docs.vllm.ai/en/latest/models/spec_decode.html" target="_blank" class="learn-link">ğŸ“„ vLLM Speculative Decoding ë¬¸ì„œ</a>
                <a href="https://huggingface.co/blog/assisted-generation" target="_blank" class="learn-link">ğŸ“ Hugging Face Assisted Generation ê°€ì´ë“œ</a>
            </div>
        </section>
    </main>

        <div id="kaitrust-footer"></div>

    <script>function copyCode(btn) { const codeBlock = btn.parentElement.querySelector('code'); navigator.clipboard.writeText(codeBlock.textContent).then(() => { btn.textContent = 'âœ… ë³µì‚¬ë¨!'; setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000); }); }</script>
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
<script src="/glossary/js/term-sections.js?v=20260129231616"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
