<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sparse Attention (ìŠ¤íŒŒìŠ¤ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ì–´í…ì…˜ ì—°ì‚°ì„ ì¼ë¶€ì—ë§Œ ì ìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•. O(N^2)ë¥¼ O(N log N)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥.">
    <meta name="keywords" content="Sparse Attention, ìŠ¤íŒŒìŠ¤ ì–´í…ì…˜, Longformer, BigBird, Self-Attention, AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/Sparse%20Attention/">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/Sparse%20Attention/">
    <meta property="og:title" content="Sparse Attention (ìŠ¤íŒŒìŠ¤ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ì–´í…ì…˜ ì—°ì‚°ì„ ì¼ë¶€ì—ë§Œ ì ìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•. O(N^2)ë¥¼ O(N log N)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥.">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Sparse Attention (ìŠ¤íŒŒìŠ¤ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ì–´í…ì…˜ ì—°ì‚°ì„ ì¼ë¶€ì—ë§Œ ì ìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•. O(N^2)ë¥¼ O(N log N)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥.">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "DefinedTerm", "name": "Sparse Attention", "description": "ì–´í…ì…˜ ì—°ì‚°ì„ ì¼ë¶€ì—ë§Œ ì ìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•. O(N^2)ë¥¼ O(N log N)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥.", "inDefinedTermSet": {"@type": "DefinedTermSet", "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „", "url": "https://glossary.kaitrust.ai/"}}
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container { max-width: 900px; margin: 0 auto; padding: 120px 2rem 4rem; position: relative; z-index: 1; }
        .breadcrumb { display: flex; align-items: center; gap: 0.5rem; margin-bottom: 2rem; font-size: 0.9rem; flex-wrap: wrap; }
        .breadcrumb a { color: #64748b; text-decoration: none; transition: color 0.2s; }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header { background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6)); border: 1px solid rgba(168, 85, 247, 0.2); border-radius: 24px; padding: 3rem; margin-bottom: 2rem; }
        .term-category-badge { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.2); border-radius: 20px; font-size: 0.85rem; color: #a855f7; margin-bottom: 1rem; }
        .term-title { font-family: 'Orbitron', sans-serif; font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem; background: linear-gradient(135deg, #ffffff, #a855f7); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .term-english { font-size: 1.2rem; color: #94a3b8; margin-bottom: 1.5rem; }
        .term-description { font-size: 1.1rem; line-height: 1.8; color: #e2e8f0; }
        .term-actions { display: flex; gap: 1rem; margin-top: 2rem; flex-wrap: wrap; }
        .term-action-btn { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1.5rem; border-radius: 12px; font-size: 0.9rem; text-decoration: none; transition: all 0.3s; }
        .btn-primary { background: linear-gradient(135deg, #a855f7, #6366f1); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3); }
        .btn-secondary { background: rgba(255, 255, 255, 0.1); color: #e2e8f0; border: 1px solid rgba(255, 255, 255, 0.2); }
        @media (max-width: 768px) { .term-detail-container { padding: 100px 1rem 2rem; } .term-detail-header { padding: 2rem 1.5rem; } .term-title { font-size: 1.8rem; } }

        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .section-content p:last-child { margin-bottom: 0; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; line-height: 1.6; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; font-size: 0.85rem; transition: all 0.2s; }
        .copy-btn:hover { background: rgba(168, 85, 247, 0.5); }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0, 0, 0, 0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; font-size: 0.95rem; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.1); border: 1px solid rgba(168, 85, 247, 0.3); border-radius: 20px; color: #a855f7; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; }
        .related-term-link:hover { background: rgba(168, 85, 247, 0.2); transform: translateY(-2px); }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; transition: all 0.2s; }
        .learn-link:hover { background: rgba(0, 0, 0, 0.3); transform: translateX(5px); }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css">

</head>
<body>
    <div class="particle-container" id="particles"></div>
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a><span>â€º</span>
            <span class="current">Sparse Attention</span>
        </nav>

        <article class="term-detail-header">
            <div class="term-category-badge"><span>ğŸ¤–</span><span>AI/ML</span></div>
            <h1 class="term-title">Sparse Attention</h1>
            <p class="term-english">ìŠ¤íŒŒìŠ¤ ì–´í…ì…˜</p>
            <div class="term-description">
                <p>ì–´í…ì…˜ ì—°ì‚°ì„ ì¼ë¶€ì—ë§Œ ì ìš©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ê¸°ë²•. O(N^2)ë¥¼ O(N log N)ìœ¼ë¡œ ì¤„ì—¬ ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥.</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°</a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">ğŸ¤– AI/ML ë”ë³´ê¸°</a>
            </div>
        </article>

        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>Sparse Attentionì€ í‘œì¤€ Self-Attentionì˜ ëª¨ë“  ìœ„ì¹˜ ìŒ(N^2)ì´ ì•„ë‹Œ, ì„ íƒëœ ìœ„ì¹˜ ìŒì—ë§Œ ì–´í…ì…˜ì„ ê³„ì‚°í•˜ëŠ” íš¨ìœ¨ì ì¸ ê¸°ë²•ì…ë‹ˆë‹¤. 4096 í† í° ì‹œí€€ìŠ¤ì—ì„œ Full Attentionì€ ì•½ 1,670ë§Œ ë²ˆ(4096^2) ì—°ì‚°í•˜ì§€ë§Œ, Sparse Attentionì€ ìœˆë„ìš° í¬ê¸° 256 ê¸°ì¤€ ì•½ 100ë§Œ ë²ˆìœ¼ë¡œ ì¤„ì—¬ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì–´í…ì…˜ í–‰ë ¬ì˜ ëŒ€ë¶€ë¶„ì´ 0(sparse)ì¸ ê²ƒì—ì„œ ì´ë¦„ì´ ìœ ë˜í–ˆìŠµë‹ˆë‹¤.</p>
                <p>2019ë…„ OpenAIì˜ "Generating Long Sequences with Sparse Transformers" ë…¼ë¬¸ì—ì„œ ë³¸ê²©ì ìœ¼ë¡œ ì—°êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. Full Attentionì˜ O(N^2) ë³µì¡ë„ëŠ” ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ë¹„ìš©ì´ í­ë°œí•©ë‹ˆë‹¤. N=8192ì¼ ë•Œ ì–´í…ì…˜ í–‰ë ¬ë§Œ 256GB ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤(FP32 ê¸°ì¤€). GPT-2ì˜ 1024 í† í°, BERTì˜ 512 í† í° ì œí•œë„ ì´ ë•Œë¬¸ì´ì—ˆìŠµë‹ˆë‹¤. Sparse Attentionì€ O(N log N) ë˜ëŠ” O(N sqrt(N))ìœ¼ë¡œ ë³µì¡ë„ë¥¼ ë‚®ì¶¥ë‹ˆë‹¤.</p>
                <p>ëŒ€í‘œì  íŒ¨í„´ìœ¼ë¡œ Local Attention(ì£¼ë³€ kê°œ í† í°ë§Œ attend), Strided Attention(ì¼ì • ê°„ê²©ìœ¼ë¡œ attend), Global Attention(íŠ¹ì • í† í°ì´ ì „ì²´ì™€ ì—°ê²°)ì´ ìˆìŠµë‹ˆë‹¤. LongformerëŠ” Local+Global ì¡°í•©ìœ¼ë¡œ 4096 í† í°ê¹Œì§€ ì§€ì›í•˜ê³ , BigBirdëŠ” Local+Global+Randomì„ ì¡°í•©í•´ ì´ë¡ ì  í‘œí˜„ë ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. Mistral 7Bì˜ Sliding Window Attention(4096)ë„ íš¨ìœ¨ì ì¸ Sparseì˜ ì¼ì¢…ì…ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” Ring Attentionìœ¼ë¡œ ì—¬ëŸ¬ GPUì— ì‹œí€€ìŠ¤ë¥¼ ë¶„ì‚° ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•ë„ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.</p>
                <p>ê¸´ ë¬¸ì„œ ìš”ì•½, ë²•ë¥  ê³„ì•½ì„œ ë¶„ì„, ì˜ë£Œ ê¸°ë¡ ì²˜ë¦¬, ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ì´í•´ ë“± ê¸´ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œ ì‘ì—…ì— í•„ìˆ˜ì…ë‹ˆë‹¤. ë‹¨, ì»¤ìŠ¤í…€ CUDA ì»¤ë„ì´ í•„ìš”í•´ êµ¬í˜„ì´ ë³µì¡í•˜ê³  GPU ìµœì í™”ê°€ ì–´ë µìŠµë‹ˆë‹¤. Flash Attentionì€ ì•Œê³ ë¦¬ì¦˜ì  Sparsity ëŒ€ì‹  ë©”ëª¨ë¦¬ ì ‘ê·¼ íŒ¨í„´ ìµœì í™”(IO-aware)ë¡œ íš¨ìœ¨ì„ ë†’ì´ëŠ” ë³´ì™„ì  ì ‘ê·¼ì´ë©°, ë‘ ê¸°ë²•ì„ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì‹œë„ˆì§€ê°€ ë‚©ë‹ˆë‹¤.</p>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python">import torch
import torch.nn.functional as F

def create_local_attention_mask(seq_len, window_size):
    """Local Attention: ì£¼ë³€ window_size í† í°ì—ë§Œ attend"""
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2 + 1)
        mask[i, start:end] = 1
    return mask

def create_strided_attention_mask(seq_len, stride):
    """Strided Attention: stride ê°„ê²©ìœ¼ë¡œ attend"""
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        # ê°™ì€ stride ë¸”ë¡ ë‚´ ìœ„ì¹˜ë“¤
        block_start = (i // stride) * stride
        block_end = min(block_start + stride, seq_len)
        mask[i, block_start:block_end] = 1
        # ëª¨ë“  stride ìœ„ì¹˜
        mask[i, ::stride] = 1
    return mask

def create_bigbird_mask(seq_len, window_size, num_global, num_random):
    """BigBird: Local + Global + Random"""
    mask = create_local_attention_mask(seq_len, window_size)
    # Global tokens (ì²˜ìŒ ëª‡ ê°œ í† í°ì´ ëª¨ë“  ìœ„ì¹˜ì™€ ì—°ê²°)
    mask[:num_global, :] = 1
    mask[:, :num_global] = 1
    # Random connections
    for i in range(seq_len):
        random_indices = torch.randperm(seq_len)[:num_random]
        mask[i, random_indices] = 1
    return mask

# ì‚¬ìš© ì˜ˆì‹œ
seq_len = 1024
window_size = 64
local_mask = create_local_attention_mask(seq_len, window_size)
print(f"Local Attention ì—°ê²° ìˆ˜: {local_mask.sum().item():,.0f}")  # ~65K
print(f"Full Attention ì—°ê²° ìˆ˜: {seq_len * seq_len:,}")  # 1,048,576

# Hugging Face Longformer ì‚¬ìš©
from transformers import LongformerModel, LongformerTokenizer

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
model = LongformerModel.from_pretrained('allenai/longformer-base-4096')

# ìµœëŒ€ 4096 í† í°ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥ (BERTëŠ” 512)
text = "This is a very long document... " * 500
inputs = tokenizer(text, return_tensors='pt', max_length=4096, truncation=True)

# global_attention_mask: 1ì¸ ìœ„ì¹˜ëŠ” ëª¨ë“  í† í°ê³¼ attend
global_attention_mask = torch.zeros_like(inputs['input_ids'])
global_attention_mask[:, 0] = 1  # [CLS] í† í°ë§Œ global

outputs = model(**inputs, global_attention_mask=global_attention_mask)

# === Sliding Window Attention (Mistral ìŠ¤íƒ€ì¼) ===
def create_sliding_window_mask(seq_len, window_size):
    """Sliding Window: ê° í† í°ì´ ì•ì˜ window_size í† í°ë§Œ attend"""
    mask = torch.zeros(seq_len, seq_len)
    for i in range(seq_len):
        start = max(0, i - window_size + 1)
        mask[i, start:i+1] = 1  # causal + window
    return mask

# Mistral 7B: window_size=4096
# 128K í† í°ë„ 4096 ìœˆë„ìš°ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬
sliding_mask = create_sliding_window_mask(8192, 4096)
print(f"Sliding Window ì—°ê²° ìˆ˜: {sliding_mask.sum().item():,.0f}")  # ~33M
print(f"Full Attention ì—°ê²° ìˆ˜: {8192 * 8192:,}")  # 67M

# === ë³µì¡ë„ ë¹„êµ ===
def attention_complexity_comparison(seq_len, window_size=256, stride=256):
    """ê° Sparse íŒ¨í„´ì˜ ì—°ì‚° íšŸìˆ˜ ë¹„êµ"""
    full = seq_len * seq_len
    local = seq_len * window_size
    strided = seq_len * (window_size + seq_len // stride)
    sliding = seq_len * min(window_size, seq_len)

    print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len:,}")
    print(f"Full Attention: {full:,} ({full/1e6:.1f}M)")
    print(f"Local Attention: {local:,} ({local/1e6:.1f}M) - {local/full*100:.1f}%")
    print(f"Strided Attention: {strided:,} ({strided/1e6:.1f}M) - {strided/full*100:.1f}%")
    print(f"Sliding Window: {sliding:,} ({sliding/1e6:.1f}M) - {sliding/full*100:.1f}%")

attention_complexity_comparison(16384, window_size=512)

# === BigBird íŒ¨í„´ (Local + Global + Random) ===
def create_bigbird_mask(seq_len, window_size=64, num_global=64, num_random=64):
    """BigBird: ì´ë¡ ì  í‘œí˜„ë ¥ì„ ìœ ì§€í•˜ë©´ì„œ íš¨ìœ¨ì„± í™•ë³´"""
    mask = torch.zeros(seq_len, seq_len)

    for i in range(seq_len):
        # Local attention (ì£¼ë³€ í† í°)
        start = max(0, i - window_size // 2)
        end = min(seq_len, i + window_size // 2)
        mask[i, start:end] = 1

        # Global attention (ì²˜ìŒ num_global í† í°ì´ ì „ì²´ì™€ ì—°ê²°)
        if i < num_global:
            mask[i, :] = 1
            mask[:, i] = 1

        # Random attention (ë¬´ì‘ìœ„ ì—°ê²°ë¡œ ì´ë¡ ì  í‘œí˜„ë ¥ ìœ ì§€)
        random_indices = torch.randint(0, seq_len, (num_random,))
        mask[i, random_indices] = 1

    return mask

# === Ring Attention (ë¶„ì‚° ì²˜ë¦¬) ===
# ì—¬ëŸ¬ GPUì— ì‹œí€€ìŠ¤ë¥¼ ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬
# ê° GPUê°€ ìì‹ ì˜ ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì „ë‹¬
# 1M í† í° ì´ìƒì˜ ì´ˆì¥ë¬¸ ì²˜ë¦¬ ê°€ëŠ¥

# === ì‹¤ì œ ëª¨ë¸ì—ì„œì˜ ì ìš© ===
# GPT-4: 128K ì»¨í…ìŠ¤íŠ¸ (ë‚´ë¶€ ìµœì í™”)
# Claude: 200K ì»¨í…ìŠ¤íŠ¸ (Ring Attention ì¶”ì •)
# Mistral 7B: 32K (Sliding Window 4096)
# Longformer: 4096 (Local + Global)
# BigBird: 4096 (Local + Global + Random)

# === Attention Sink í˜„ìƒ ===
# ì²« ë²ˆì§¸ í† í°ì— ì–´í…ì…˜ì´ ì§‘ì¤‘ë˜ëŠ” í˜„ìƒ
# StreamingLLMì—ì„œ ë°œê²¬: ì²« í† í°ì„ ìœ ì§€í•˜ë©´ ë¬´í•œ ìƒì„± ê°€ëŠ¥

# === ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ===
# ì‹œí€€ìŠ¤ ê¸¸ì´ 16Kì—ì„œ:
# - Full Attention: ë©”ëª¨ë¦¬ OOM (A100 40GB)
# - Sparse Attention (window=512): 8GB
# - Flash Attention: 12GB (ë” ì •í™•, ë” ë¹ ë¦„)
# - Sparse + Flash: 6GB (ìµœì )

def memory_usage_comparison(seq_lengths):
    """ë‹¤ì–‘í•œ ì–´í…ì…˜ ë°©ì‹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ"""
    for n in seq_lengths:
        full = n * n * 4 / 1e9  # FP32, GB
        sparse_512 = n * 512 * 4 / 1e9
        print(f"N={n}: Full={full:.1f}GB, Sparse={sparse_512:.3f}GB")

memory_usage_comparison([4096, 8192, 16384, 32768])

# === êµ¬í˜„ ë„ì „ ê³¼ì œ ===
# 1. ì»¤ìŠ¤í…€ CUDA ì»¤ë„ í•„ìš” (PyTorch ê¸°ë³¸ X)
# 2. Sparse íŒ¨í„´ì´ GPUì—ì„œ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ
# 3. ì»´íŒŒì¼ëŸ¬ ìµœì í™” ì–´ë ¤ì›€
# Flash Attentionì´ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒí•œ ì´ìœ !

# === ìµœì‹  íŠ¸ë Œë“œ ===
# - Linear Attention: softmax ëŒ€ì‹  ì»¤ë„ ê·¼ì‚¬
# - Performer: FAVOR+ ë©”ì»¤ë‹ˆì¦˜
# - Linformer: ì €ë­í¬ ê·¼ì‚¬
# ëª¨ë‘ O(n^2) -> O(n) ëª©í‘œ

# === Dilated Attention (LongNet) ===
# ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” dilationìœ¼ë¡œ 1B í† í° ì²˜ë¦¬
# ê°€ê¹Œìš´ í† í°ì€ dense, ë¨¼ í† í°ì€ sparseí•˜ê²Œ ì²˜ë¦¬

# === ì‹¤ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===
# 1. ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 2K ì´í•˜ë©´ Full Attentionìœ¼ë¡œ ì¶©ë¶„
# 2. 2K-16K: Sliding Window ë˜ëŠ” Flash Attention
# 3. 16K-100K: BigBird, Longformer + Flash Attention
# 4. 100K+: Ring Attention, LongNet ê³ ë ¤
# 5. GPU ë©”ëª¨ë¦¬ì™€ latency ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì„ íƒ

# === ìµœì‹  ëª¨ë¸ë³„ Attention ì „ëµ ===
# | ëª¨ë¸ | ì»¨í…ìŠ¤íŠ¸ | ì „ëµ |
# |-----|---------|-----|
# | GPT-4 Turbo | 128K | ë‚´ë¶€ ìµœì í™” |
# | Claude 3 | 200K | Ring Attention |
# | Gemini 1.5 | 1M | ë©€í‹°ëª¨ë‹¬ ìµœì í™” |</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ë…¼ì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"ë²•ë¥  ë¬¸ì„œê°€ ë³´í†µ 3ë§Œ í† í°ì´ë¼ BERTë¡œëŠ” ì•ˆ ë©ë‹ˆë‹¤. Longformer ì“°ë©´ 4096 í† í°ê¹Œì§€ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥í•˜ê³ , ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ  ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ë¬¸ë§¥ íŒŒì•…ì´ í›¨ì”¬ ì¢‹ì•„ìš”. Local + Global Attention ë•ë¶„ì´ì£ ."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"Sparse Attentionì€ Full Attentionì˜ O(N^2) ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ëª¨ë“  ìŒì´ ì•„ë‹ˆë¼ Local(ì£¼ë³€), Global(í•µì‹¬ í† í°), Random ì—°ê²°ë§Œ ê³„ì‚°í•´ì„œ O(N)ì´ë‚˜ O(N log N)ìœ¼ë¡œ ì¤„ì´ì£ . BigBird, Longformerê°€ ëŒ€í‘œì ì´ê³ , ìµœê·¼ Mistralì˜ Sliding Windowë„ ê°™ì€ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ëª¨ë¸ ì„ íƒ íšŒì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"128K ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë©´ Sparse Attention ëª¨ë¸ì´ë‚˜ Flash Attentionì´ ì ìš©ëœ ëª¨ë¸ì„ ì¨ì•¼ í•´ìš”. Claudeë‚˜ GPT-4 TurboëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ìµœì í™”ê°€ ë˜ì–´ ìˆê³ , ì˜¤í”ˆì†ŒìŠ¤ë©´ Mistralì´ Sliding Windowë¡œ íš¨ìœ¨ì ì…ë‹ˆë‹¤."</blockquote>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>Window sizeë¥¼ ë„ˆë¬´ ì‘ê²Œ ì„¤ì •</strong>
                        <p>Local Attention ìœˆë„ìš°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë©€ë¦¬ ë–¨ì–´ì§„ ê´€ê³„ë¥¼ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ ìƒí˜¸ì°¸ì¡°(ì˜ˆ: "ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼")ê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>Global tokenì„ ì ì ˆíˆ ì„¤ì •í•˜ì§€ ì•ŠìŒ</strong>
                        <p>Longformerì—ì„œ [CLS]ë‚˜ ì§ˆë¬¸ í† í°ì„ globalë¡œ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ìš”ì•½/QA ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤. íƒœìŠ¤í¬ì— ë§ëŠ” global ìœ„ì¹˜ ì„¤ì •ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ë°©ë²•</strong>
                        <p>Local windowëŠ” ìµœì†Œ 256~512, ë¬¸ì„œ ë¶„ë¥˜ëŠ” [CLS]ë¥¼ globalë¡œ, QAëŠ” ì§ˆë¬¸ í† í° ì „ì²´ë¥¼ globalë¡œ ì„¤ì •í•˜ì„¸ìš”. í•„ìš”ì‹œ Flash Attentionê³¼ ì¡°í•©í•´ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Self-Attention/" class="related-term-link">Self-Attention</a>
                <a href="/ko/term/Flash%20Attention/" class="related-term-link">Flash Attention</a>
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/Longformer/" class="related-term-link">Longformer</a>
                <a href="/ko/term/Multi-Head%20Attention/" class="related-term-link">Multi-Head Attention</a>
                <a href="/ko/term/Context%20Length/" class="related-term-link">Context Length</a>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/1904.10509" target="_blank" class="learn-link">ğŸ“ ë…¼ë¬¸: Generating Long Sequences with Sparse Transformers (OpenAI)</a>
                <a href="https://arxiv.org/abs/2004.05150" target="_blank" class="learn-link">ğŸ“ ë…¼ë¬¸: Longformer - The Long-Document Transformer</a>
                <a href="https://huggingface.co/docs/transformers/model_doc/longformer" target="_blank" class="learn-link">ğŸ“„ Hugging Face Longformer ë¬¸ì„œ</a>
            </div>
        </section>
    </main>

    <!-- Engagement Kit -->
    <div id="kaitrust-engagement-kit"></div>
    <link rel="stylesheet" href="/components/engagement-kit/engagement-kit.css?v=20260130015843">
    <script src="/components/engagement-kit/engagement-kit.js?v=20260130015843"></script>

        <div id="kaitrust-footer"></div>

    <script>
    function copyCode(btn) { const codeBlock = btn.parentElement.querySelector('code'); navigator.clipboard.writeText(codeBlock.textContent).then(() => { btn.textContent = 'âœ… ë³µì‚¬ë¨!'; setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000); }); }
    </script>
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
<script src="/glossary/js/term-sections.js"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
