<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Attention (ì…€í”„ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìš”ì†Œ ê°„ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. Transformerì˜ í•µì‹¬. Query, Key, Valueë¡œ ë¬¸ë§¥ì„ íŒŒì•….">
    <meta name="keywords" content="Self-Attention, ì…€í”„ ì–´í…ì…˜, Transformer, QKV, Multi-Head Attention, AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/Self-Attention/">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/Self-Attention/">
    <meta property="og:title" content="Self-Attention (ì…€í”„ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìš”ì†Œ ê°„ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. Transformerì˜ í•µì‹¬. Query, Key, Valueë¡œ ë¬¸ë§¥ì„ íŒŒì•….">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Self-Attention (ì…€í”„ ì–´í…ì…˜) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìš”ì†Œ ê°„ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. Transformerì˜ í•µì‹¬. Query, Key, Valueë¡œ ë¬¸ë§¥ì„ íŒŒì•….">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <!-- Structured Data (JSON-LD) -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "DefinedTerm",
        "name": "Self-Attention",
        "description": "ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìš”ì†Œ ê°„ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. Transformerì˜ í•µì‹¬. Query, Key, Valueë¡œ ë¬¸ë§¥ì„ íŒŒì•….",
        "inDefinedTermSet": {
            "@type": "DefinedTermSet",
            "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „",
            "url": "https://glossary.kaitrust.ai/"
        }
    }
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Common CSS -->
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container { max-width: 900px; margin: 0 auto; padding: 120px 2rem 4rem; position: relative; z-index: 1; }
        .breadcrumb { display: flex; align-items: center; gap: 0.5rem; margin-bottom: 2rem; font-size: 0.9rem; flex-wrap: wrap; }
        .breadcrumb a { color: #64748b; text-decoration: none; transition: color 0.2s; }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header { background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6)); border: 1px solid rgba(168, 85, 247, 0.2); border-radius: 24px; padding: 3rem; margin-bottom: 2rem; position: relative; overflow: hidden; }
        .term-category-badge { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.2); border-radius: 20px; font-size: 0.85rem; color: #a855f7; margin-bottom: 1rem; }
        .term-title { font-family: 'Orbitron', sans-serif; font-size: 2.5rem; font-weight: 700; margin-bottom: 0.5rem; background: linear-gradient(135deg, #ffffff, #a855f7); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
        .term-english { font-size: 1.2rem; color: #94a3b8; margin-bottom: 1.5rem; }
        .term-description { font-size: 1.1rem; line-height: 1.8; color: #e2e8f0; }
        .term-actions { display: flex; gap: 1rem; margin-top: 2rem; flex-wrap: wrap; }
        .term-action-btn { display: inline-flex; align-items: center; gap: 0.5rem; padding: 0.75rem 1.5rem; border-radius: 12px; font-size: 0.9rem; text-decoration: none; transition: all 0.3s; }
        .btn-primary { background: linear-gradient(135deg, #a855f7, #6366f1); color: white; }
        .btn-primary:hover { transform: translateY(-2px); box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3); }
        .btn-secondary { background: rgba(255, 255, 255, 0.1); color: #e2e8f0; border: 1px solid rgba(255, 255, 255, 0.2); }
        .btn-secondary:hover { background: rgba(255, 255, 255, 0.2); }
        @media (max-width: 768px) { .term-detail-container { padding: 100px 1rem 2rem; } .term-detail-header { padding: 2rem 1.5rem; } .term-title { font-size: 1.8rem; } }

        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .section-content p:last-child { margin-bottom: 0; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; line-height: 1.6; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; font-size: 0.85rem; transition: all 0.2s; }
        .copy-btn:hover { background: rgba(168, 85, 247, 0.5); }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0, 0, 0, 0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; font-size: 0.95rem; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168, 85, 247, 0.1); border: 1px solid rgba(168, 85, 247, 0.3); border-radius: 20px; color: #a855f7; text-decoration: none; font-size: 0.9rem; transition: all 0.2s; }
        .related-term-link:hover { background: rgba(168, 85, 247, 0.2); transform: translateY(-2px); }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0, 0, 0, 0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; transition: all 0.2s; }
        .learn-link:hover { background: rgba(0, 0, 0, 0.3); transform: translateX(5px); }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260130002423">

</head>
<body>
    <div class="particle-container" id="particles"></div>

        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a><span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a><span>â€º</span>
            <span class="current">Self-Attention</span>
        </nav>

        <article class="term-detail-header">
            <div class="term-category-badge"><span>ğŸ¤–</span><span>AI/ML</span></div>
            <h1 class="term-title">Self-Attention</h1>
            <p class="term-english">ì…€í”„ ì–´í…ì…˜</p>
            <div class="term-description">
                <p>ì‹œí€€ìŠ¤ ë‚´ ëª¨ë“  ìš”ì†Œ ê°„ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜. Transformerì˜ í•µì‹¬. Query, Key, Valueë¡œ ë¬¸ë§¥ì„ íŒŒì•….</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°</a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">ğŸ¤– AI/ML ë”ë³´ê¸°</a>
            </div>
        </article>

        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>Self-Attention(ìê¸° ì£¼ì˜)ì€ ì‹œí€€ìŠ¤ì˜ ê° ìš”ì†Œê°€ ê°™ì€ ì‹œí€€ìŠ¤ ë‚´ ë‹¤ë¥¸ ëª¨ë“  ìš”ì†Œì™€ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. "The cat sat on the mat because it was tired"ì—ì„œ "it"ì´ "cat"ì„ ê°€ë¦¬í‚¨ë‹¤ëŠ” ê²ƒì„ íŒŒì•…í•˜ë ¤ë©´, ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ì˜ ê´€ê³„ë¥¼ ë´ì•¼ í•©ë‹ˆë‹¤. Self-Attentionì´ ë°”ë¡œ ì´ ì—­í• ì„ í•©ë‹ˆë‹¤.</p>
                <p>2017ë…„ "Attention Is All You Need" ë…¼ë¬¸ì—ì„œ Transformerì™€ í•¨ê»˜ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ RNNì€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•´ì„œ ë³‘ë ¬í™”ê°€ ì–´ë µê³  ê¸´ ì˜ì¡´ì„± í•™ìŠµì´ í˜ë“¤ì—ˆìŠµë‹ˆë‹¤. Self-Attentionì€ ëª¨ë“  ìœ„ì¹˜ë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ë¯€ë¡œ ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ê³ , ê±°ë¦¬ì— ê´€ê³„ì—†ì´ ê´€ê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.</p>
                <p>í•µì‹¬ì€ Query(Q), Key(K), Value(V) ë³€í™˜ì…ë‹ˆë‹¤. ì…ë ¥ Xì— ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ Q, K, Vë¥¼ ë§Œë“¤ê³ , Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * Vë¡œ ê³„ì‚°í•©ë‹ˆë‹¤. Qì™€ Kì˜ ë‚´ì ì´ ë†’ìœ¼ë©´ ê´€ë ¨ì„±ì´ ë†’ì•„ í•´ë‹¹ Vì— ë” ë§ì€ ê°€ì¤‘ì¹˜ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.</p>
                <p>GPT, BERT, T5 ë“± ëª¨ë“  LLMì˜ ê¸°ë°˜ì´ë©°, Vision Transformer(ViT)ë¡œ ì´ë¯¸ì§€ì—ë„ ì ìš©ë©ë‹ˆë‹¤. Multi-Head Attentionì€ ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•´ ë‹¤ì–‘í•œ ê´€ê³„ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ê¸¸ì´ Nì— ëŒ€í•´ O(N^2) ë³µì¡ë„ê°€ ë‹¨ì ì´ë¼ Sparse Attention, Flash Attention ë“± ìµœì í™” ê¸°ë²•ì´ ì—°êµ¬ë©ë‹ˆë‹¤.</p>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    """Scaled Dot-Product Self-Attention"""
    def __init__(self, embed_dim, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Query, Key, Value ë³€í™˜ í–‰ë ¬
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # Q, K, V ê³„ì‚°
        Q = self.W_q(x)  # (batch, seq, embed)
        K = self.W_k(x)
        V = self.W_v(x)

        # Attention Score: QK^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        # scores: (batch, seq, seq)

        # ë§ˆìŠ¤í‚¹ (ì˜µì…˜: causal mask for decoder)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        # Softmaxë¡œ í™•ë¥  ë³€í™˜
        attention_weights = F.softmax(scores, dim=-1)

        # Valueì— ê°€ì¤‘ì¹˜ ì ìš©
        output = torch.matmul(attention_weights, V)
        output = self.W_o(output)

        return output, attention_weights

# ì‚¬ìš© ì˜ˆì‹œ
embed_dim = 512
seq_len = 10
batch_size = 2

x = torch.randn(batch_size, seq_len, embed_dim)
attention = SelfAttention(embed_dim)
output, weights = attention(x)

print(f"ì…ë ¥: {x.shape}")          # (2, 10, 512)
print(f"ì¶œë ¥: {output.shape}")     # (2, 10, 512)
print(f"ì–´í…ì…˜ ê°€ì¤‘ì¹˜: {weights.shape}")  # (2, 10, 10)

# Causal Mask (Decoderìš© - ë¯¸ë˜ í† í° ì°¨ë‹¨)
causal_mask = torch.tril(torch.ones(seq_len, seq_len))
print(f"Causal Mask:\n{causal_mask}")
# 1 0 0 0 ...
# 1 1 0 0 ...
# 1 1 1 0 ...

output_masked, _ = attention(x, mask=causal_mask)

# === Multi-Head Attention êµ¬í˜„ ===
class MultiHeadAttention(nn.Module):
    """Multi-Head Self-Attention (ì—¬ëŸ¬ í—¤ë“œ ë³‘ë ¬ ì²˜ë¦¬)"""
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == embed_dim

        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # Q, K, V ê³„ì‚° í›„ í—¤ë“œë³„ë¡œ ë¶„ë¦¬
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # (batch, heads, seq, head_dim)

        # ê° í—¤ë“œì—ì„œ ë…ë¦½ì ìœ¼ë¡œ Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)

        # í—¤ë“œ ê²°í•©
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        return self.W_o(output)

# === PyTorch ë‚´ì¥ ì‚¬ìš© (ê¶Œì¥) ===
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
output, attn_weights = mha(x, x, x)  # Self-Attention: Q=K=V

# === Flash Attention (ë©”ëª¨ë¦¬ íš¨ìœ¨) ===
# pip install flash-attn
from flash_attn import flash_attn_func

# Flash Attentionì€ O(N) ë©”ëª¨ë¦¬ë¡œ O(N^2) ì—°ì‚°ì„ ìˆ˜í–‰
# 128K ì»¨í…ìŠ¤íŠ¸ë„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  í•µì‹¬ ê¸°ìˆ 

# === Attention ê°€ì¤‘ì¹˜ ì‹œê°í™” ===
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attention_weights, tokens):
    """ì–´í…ì…˜ íˆíŠ¸ë§µ ì‹œê°í™”"""
    plt.figure(figsize=(10, 10))
    sns.heatmap(
        attention_weights.detach().numpy(),
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='viridis'
    )
    plt.title("Self-Attention Weights")
    plt.show()

# === Cross-Attention vs Self-Attention ===
# Self-Attention: Q, K, V ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ ìœ ë˜
# Cross-Attention: QëŠ” í•œ ì…ë ¥, K/VëŠ” ë‹¤ë¥¸ ì…ë ¥ì—ì„œ
# ì˜ˆ: Encoder-Decoderì—ì„œ Decoderê°€ Encoder ì¶œë ¥ ì°¸ì¡°

class CrossAttention(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)

    def forward(self, query_input, key_value_input):
        Q = self.W_q(query_input)      # Decoder ìƒíƒœ
        K = self.W_k(key_value_input)  # Encoder ì¶œë ¥
        V = self.W_v(key_value_input)  # Encoder ì¶œë ¥
        # ì´í•˜ ë™ì¼í•œ attention ê³„ì‚°</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ëª¨ë¸ ì•„í‚¤í…ì²˜ ë…¼ì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"Transformer ë ˆì´ì–´ í•˜ë‚˜ì— Multi-Head Self-Attentionì´ ë“¤ì–´ê°€ëŠ”ë°, í—¤ë“œ ìˆ˜ê°€ 8ì´ë©´ 8ê°€ì§€ ë‹¤ë¥¸ ê´€ê³„ íŒ¨í„´ì„ ë³‘ë ¬ë¡œ í•™ìŠµí•´ìš”. ì–´ë–¤ í—¤ë“œëŠ” êµ¬ë¬¸ ê´€ê³„, ì–´ë–¤ í—¤ë“œëŠ” ìƒí˜¸ì°¸ì¡°(coreference)ë¥¼ ë°°ìš°ë”ë¼ê³ ìš”."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"Self-Attentionì€ Q, K, V ì„¸ ê°€ì§€ ë³€í™˜ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. Qì™€ Kì˜ ë‚´ì ìœ¼ë¡œ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , softmaxë¡œ ì •ê·œí™”í•œ ë’¤ Vì— ê°€ì¤‘í•©ì„ ì ìš©í•©ë‹ˆë‹¤. sqrt(d_k)ë¡œ ë‚˜ëˆ„ëŠ” ê±´ ë‚´ì  ê°’ì´ ì»¤ì ¸ì„œ softmaxê°€ ê·¹ë‹¨ì ì´ ë˜ëŠ” ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ì„±ëŠ¥ ìµœì í™” ë…¼ì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"Self-Attentionì´ O(N^2)ì´ë¼ ê¸´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë©”ëª¨ë¦¬ í­ë°œì´ ì¼ì–´ë‚˜ìš”. Flash Attention ì“°ë©´ ë©”ëª¨ë¦¬ë¥¼ 90% ì¤„ì´ë©´ì„œ ì†ë„ë„ 2-3ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤. 128K ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ì—ëŠ” í•„ìˆ˜ì˜ˆìš”."</blockquote>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ìŠ¤ì¼€ì¼ë§ ì—†ì´ ë‚´ì  ê³„ì‚°</strong>
                        <p>sqrt(d_k)ë¡œ ë‚˜ëˆ„ì§€ ì•Šìœ¼ë©´ d_kê°€ í´ ë•Œ ë‚´ì  ê°’ì´ ë§¤ìš° ì»¤ì ¸ì„œ softmaxê°€ ê±°ì˜ one-hotì´ ë©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì†Œì‹¤ë˜ì–´ í•™ìŠµì´ ì•ˆ ë©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>Decoderì—ì„œ causal mask ëˆ„ë½</strong>
                        <p>ì–¸ì–´ ëª¨ë¸ í•™ìŠµ ì‹œ ë¯¸ë˜ í† í°ì„ ë³¼ ìˆ˜ ìˆìœ¼ë©´ ì •ë‹µì„ ë¯¸ë¦¬ ì•Œê²Œ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ í•˜ì‚¼ê° í–‰ë ¬ ë§ˆìŠ¤í¬ë¡œ ë¯¸ë˜ë¥¼ ì°¨ë‹¨í•˜ì„¸ìš”.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ë°©ë²•</strong>
                        <p>PyTorchì˜ nn.MultiheadAttentionì„ ì‚¬ìš©í•˜ë©´ ìŠ¤ì¼€ì¼ë§, ë§ˆìŠ¤í‚¹ì´ ìë™ ì²˜ë¦¬ë©ë‹ˆë‹¤. ì»¤ìŠ¤í…€ êµ¬í˜„ ì‹œ ë°˜ë“œì‹œ / sqrt(d_k)ì™€ attn_mask íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.</p>
                    </div>
                </div>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/Multi-Head%20Attention/" class="related-term-link">Multi-Head Attention</a>
                <a href="/ko/term/Sparse%20Attention/" class="related-term-link">Sparse Attention</a>
                <a href="/ko/term/Flash%20Attention/" class="related-term-link">Flash Attention</a>
                <a href="/ko/term/Softmax/" class="related-term-link">Softmax</a>
                <a href="/ko/term/Positional%20Encoding/" class="related-term-link">Positional Encoding</a>
            </div>
        </section>

        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/1706.03762" target="_blank" class="learn-link">ğŸ“ ë…¼ë¬¸: Attention Is All You Need</a>
                <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" class="learn-link">ğŸ“ The Illustrated Transformer (Jay Alammar)</a>
                <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" target="_blank" class="learn-link">ğŸ“„ PyTorch MultiheadAttention ë¬¸ì„œ</a>
            </div>
        </section>
    </main>

        <div id="kaitrust-footer"></div>

    <script>
    function copyCode(btn) {
        const codeBlock = btn.parentElement.querySelector('code');
        navigator.clipboard.writeText(codeBlock.textContent).then(() => {
            btn.textContent = 'âœ… ë³µì‚¬ë¨!';
            setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000);
        });
    }
    </script>
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
<script src="/glossary/js/term-sections.js?v=20260130002423"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
