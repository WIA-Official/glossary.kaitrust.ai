<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MoE (Mixture of Experts) | KAITRUST AI ë°±ê³¼ì‚¬ì „</title>
    <meta name="description" content="ì—¬ëŸ¬ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ì•„í‚¤í…ì²˜. íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ .">
    <meta name="keywords" content="MoE, Mixture of Experts, AI ìš©ì–´, KAITRUST, AI ë°±ê³¼ì‚¬ì „, AI/ML, Sparse MoE, Gating">
    <link rel="canonical" href="https://glossary.kaitrust.ai/ko/term/MoE/">

    <!-- Open Graph -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://glossary.kaitrust.ai/ko/term/MoE/">
    <meta property="og:title" content="MoE (Mixture of Experts) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta property="og:description" content="ì—¬ëŸ¬ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ì•„í‚¤í…ì²˜. íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ .">
    <meta property="og:image" content="https://kaitrust.ai/images/og-glossary.png">
    <meta property="og:locale" content="ko_KR">
    <meta property="og:site_name" content="KAITRUST AI ë°±ê³¼ì‚¬ì „">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="MoE (Mixture of Experts) | KAITRUST AI ë°±ê³¼ì‚¬ì „">
    <meta name="twitter:description" content="ì—¬ëŸ¬ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ì•„í‚¤í…ì²˜. íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ .">
    <meta name="twitter:image" content="https://kaitrust.ai/images/og-glossary.png">

    <!-- Structured Data (JSON-LD) -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "DefinedTerm",
        "name": "MoE",
        "description": "ì—¬ëŸ¬ ì „ë¬¸ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ì•„í‚¤í…ì²˜. íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ .",
        "inDefinedTermSet": {
            "@type": "DefinedTermSet",
            "name": "KAITRUST AI ë°±ê³¼ì‚¬ì „",
            "url": "https://glossary.kaitrust.ai/"
        }
    }
    </script>

    <link rel="icon" type="image/png" href="https://kaitrust.ai/favicon.png">
    <link rel="apple-touch-icon" href="https://kaitrust.ai/favicon.png">

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;600;700;900&family=Noto+Sans+KR:wght@300;400;500;700;900&family=JetBrains+Mono:wght@400;500;600&display=swap" rel="stylesheet">

    <!-- Common CSS -->
    <link rel="stylesheet" href="/css/kaitrust-common.css">
    <link rel="stylesheet" href="/css/light-mode.css">
    <link rel="stylesheet" href="/components/ask-ai/kaitrust-ai-modal.css">

    <style>
        .term-detail-container {
            max-width: 900px;
            margin: 0 auto;
            padding: 120px 2rem 4rem;
            position: relative;
            z-index: 1;
        }
        .breadcrumb {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            flex-wrap: wrap;
        }
        .breadcrumb a {
            color: #64748b;
            text-decoration: none;
            transition: color 0.2s;
        }
        .breadcrumb a:hover { color: var(--primary); }
        .breadcrumb span { color: #64748b; }
        .breadcrumb .current { color: var(--accent); font-weight: 500; }
        .term-detail-header {
            background: linear-gradient(145deg, rgba(15, 23, 42, 0.9), rgba(30, 41, 59, 0.6));
            border: 1px solid rgba(168, 85, 247, 0.2);
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 2rem;
            position: relative;
            overflow: hidden;
        }
        .term-category-badge {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background: rgba(168, 85, 247, 0.2);
            border-radius: 20px;
            font-size: 0.85rem;
            color: #a855f7;
            margin-bottom: 1rem;
        }
        .term-title {
            font-family: 'Orbitron', sans-serif;
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            background: linear-gradient(135deg, #ffffff, #a855f7);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .term-english {
            font-size: 1.2rem;
            color: #94a3b8;
            margin-bottom: 1.5rem;
        }
        .term-description {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #e2e8f0;
        }
        .term-actions {
            display: flex;
            gap: 1rem;
            margin-top: 2rem;
            flex-wrap: wrap;
        }
        .term-action-btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.75rem 1.5rem;
            border-radius: 12px;
            font-size: 0.9rem;
            text-decoration: none;
            transition: all 0.3s;
        }
        .btn-primary {
            background: linear-gradient(135deg, #a855f7, #6366f1);
            color: white;
        }
        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 30px rgba(168, 85, 247, 0.3);
        }
        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            color: #e2e8f0;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.2);
        }
        .related-section {
            margin-top: 3rem;
        }
        .related-title {
            font-size: 1.3rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: #f1f5f9;
        }
        @media (max-width: 768px) {
            .term-detail-container { padding: 100px 1rem 2rem; }
            .term-detail-header { padding: 2rem 1.5rem; }
            .term-title { font-size: 1.8rem; }
        }

        /* ê³ ë„í™” ì„¹ì…˜ ìŠ¤íƒ€ì¼ */
        .term-section { background: rgba(15, 23, 42, 0.6); border: 1px solid rgba(168, 85, 247, 0.1); border-radius: 16px; padding: 2rem; margin-bottom: 1.5rem; }
        .section-title { font-size: 1.4rem; font-weight: 600; color: #f1f5f9; margin-bottom: 1.5rem; padding-bottom: 0.75rem; border-bottom: 1px solid rgba(168, 85, 247, 0.2); }
        .section-content p { color: #cbd5e1; line-height: 1.8; margin-bottom: 1rem; }
        .code-tabs { display: flex; gap: 0.5rem; margin-bottom: 1rem; }
        .code-tab { padding: 0.5rem 1rem; background: rgba(255,255,255,0.05); border: 1px solid rgba(255,255,255,0.1); border-radius: 8px; color: #94a3b8; cursor: pointer; }
        .code-tab.active { background: rgba(168,85,247,0.2); border-color: #a855f7; color: #a855f7; }
        .code-block { position: relative; background: #0f172a; border-radius: 12px; overflow: hidden; }
        .code-block pre { padding: 1.5rem; margin: 0; overflow-x: auto; }
        .code-block code { font-family: 'JetBrains Mono', monospace; font-size: 0.9rem; color: #e2e8f0; }
        .copy-btn { position: absolute; top: 0.75rem; right: 0.75rem; padding: 0.5rem 1rem; background: rgba(168,85,247,0.3); border: none; border-radius: 6px; color: #e2e8f0; cursor: pointer; }
        .conversation-examples { display: flex; flex-direction: column; gap: 1.5rem; }
        .conv-item { background: rgba(0,0,0,0.2); border-radius: 12px; padding: 1.25rem; }
        .conv-context { font-size: 0.9rem; color: #a855f7; font-weight: 500; margin-bottom: 0.75rem; }
        .conv-quote { color: #e2e8f0; font-style: italic; line-height: 1.7; margin: 0; padding-left: 1rem; border-left: 3px solid #a855f7; }
        .warning-list { display: flex; flex-direction: column; gap: 1rem; }
        .warning-item { display: flex; gap: 1rem; padding: 1rem; background: rgba(0,0,0,0.2); border-radius: 10px; }
        .warning-icon { font-size: 1.5rem; }
        .warning-item strong { color: #f1f5f9; display: block; margin-bottom: 0.25rem; }
        .warning-item p { color: #94a3b8; margin: 0; }
        .related-terms { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .related-term-link { padding: 0.5rem 1rem; background: rgba(168,85,247,0.1); border: 1px solid rgba(168,85,247,0.3); border-radius: 20px; color: #a855f7; text-decoration: none; }
        .learn-more { display: flex; flex-direction: column; gap: 0.75rem; }
        .learn-link { display: flex; align-items: center; gap: 0.75rem; padding: 1rem; background: rgba(0,0,0,0.2); border-radius: 10px; color: #00f5ff; text-decoration: none; }
        .spec-table { width: 100%; border-collapse: collapse; margin-top: 1rem; }
        .spec-table th, .spec-table td { padding: 0.75rem; text-align: left; border-bottom: 1px solid rgba(168,85,247,0.1); }
        .spec-table th { color: #a855f7; font-weight: 500; }
        .spec-table td { color: #cbd5e1; }
    </style>
    <link rel="stylesheet" href="/glossary/css/term-sections.css?v=20260129233538">

</head>
<body>
    <!-- Particle Background -->
    <div class="particle-container" id="particles"></div>

    <!-- Header -->
        <div id="kaitrust-header"></div>

    <main class="term-detail-container">
        <!-- Breadcrumb -->
        <nav class="breadcrumb" aria-label="Breadcrumb">
            <a href="https://kaitrust.ai">í™ˆ</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai">AI ë°±ê³¼ì‚¬ì „</a>
            <span>â€º</span>
            <a href="https://glossary.kaitrust.ai/#ai">AI/ML</a>
            <span>â€º</span>
            <span class="current">MoE</span>
        </nav>

        <!-- Term Header -->
        <article class="term-detail-header">
            <div class="term-category-badge">
                <span>ğŸ¤–</span>
                <span>AI/ML</span>
            </div>
            <h1 class="term-title">MoE</h1>
            <p class="term-english">Mixture of Experts</p>
            <div class="term-description">
                <p>ì—¬ëŸ¬ ì „ë¬¸ê°€(Expert) ë„¤íŠ¸ì›Œí¬ë¥¼ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ëŠ” ì•„í‚¤í…ì²˜. íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ëª¨ë¸ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ GPT-4, Mixtral, Gemini ë“±ì— ì ìš©.</p>
            </div>
            <div class="term-actions">
                <a href="https://glossary.kaitrust.ai" class="term-action-btn btn-primary">
                    ğŸ“š ì „ì²´ ìš©ì–´ ë³´ê¸°
                </a>
                <a href="https://glossary.kaitrust.ai/#ai" class="term-action-btn btn-secondary">
                    ğŸ¤– AI/ML ë”ë³´ê¸°
                </a>
            </div>
        </article>

        <!-- ğŸ“– ìƒì„¸ ì„¤ëª… -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“– ìƒì„¸ ì„¤ëª…</h2>
            <div class="section-content">
                <p>MoE(Mixture of Experts)ëŠ” ì—¬ëŸ¬ ê°œì˜ "ì „ë¬¸ê°€" ì„œë¸Œë„¤íŠ¸ì›Œí¬ ì¤‘ ì¼ë¶€ë§Œ ì„ íƒì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì—°ì‚° íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” í¬ì§€ë§Œ ì‹¤ì œ ì¶”ë¡  ì‹œì—ëŠ” ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ "Sparse" ëª¨ë¸ì´ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.</p>
                <p>MoEì˜ í•µì‹¬ì€ Gating Network(ë¼ìš°í„°)ì…ë‹ˆë‹¤. ê° ì…ë ¥ í† í°ì´ ë“¤ì–´ì˜¤ë©´ ë¼ìš°í„°ê°€ ì–´ë–¤ Expertë¥¼ í™œì„±í™”í• ì§€ ê²°ì •í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Mixtral 8x7BëŠ” 8ê°œ Expert ì¤‘ Top-2ë¥¼ ì„ íƒí•˜ì—¬ ì´ 45B íŒŒë¼ë¯¸í„° ì¤‘ 12Bë§Œìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Dense ëª¨ë¸ ëŒ€ë¹„ 2-4ë°° ì ì€ FLOPsë¡œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.</p>
                <p>MoEì˜ ì¥ì ì€ ëª¨ë¸ ìš©ëŸ‰(capacity) í™•ì¥ì— ìˆìŠµë‹ˆë‹¤. Expert ìˆ˜ë¥¼ ëŠ˜ë ¤ ì „ì²´ ì§€ì‹ ìš©ëŸ‰ì„ í‚¤ìš°ë©´ì„œë„ ì¶”ë¡  ë¹„ìš©ì€ ì¼ì •í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Googleì˜ Switch TransformerëŠ” 1.6T íŒŒë¼ë¯¸í„°ë¥¼ ë‹¬ì„±í–ˆê³ , Mistral Large 3ëŠ” 675B ì¤‘ 41Bë§Œ í™œì„±í™”í•©ë‹ˆë‹¤.</p>
                <p>ì‹¤ë¬´ì—ì„œ MoEëŠ” ëŒ€ê·œëª¨ LLM ì„œë¹„ìŠ¤ì˜ ë¹„ìš© íš¨ìœ¨í™”ì— í•„ìˆ˜ì…ë‹ˆë‹¤. ë‹¤ë§Œ ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ì•¼ í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì€ ì¤„ì§€ ì•Šìœ¼ë©°, Expert ê°„ ë¡œë“œ ë°¸ëŸ°ì‹±ì´ í•™ìŠµì˜ í•µì‹¬ ê³¼ì œì…ë‹ˆë‹¤. vLLM, TensorRT-LLM ë“± MoE ìµœì í™” ì„œë¹™ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ë©´ íš¨ìœ¨ì ì¸ ë°°í¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>
            </div>
            <table class="spec-table">
                <tr><th>ëª¨ë¸</th><th>ì´ íŒŒë¼ë¯¸í„°</th><th>í™œì„± íŒŒë¼ë¯¸í„°</th><th>Expert ìˆ˜</th></tr>
                <tr><td>Mixtral 8x7B</td><td>45B</td><td>12B</td><td>8 (Top-2)</td></tr>
                <tr><td>Mixtral 8x22B</td><td>141B</td><td>39B</td><td>8 (Top-2)</td></tr>
                <tr><td>Mistral Large 3</td><td>675B</td><td>41B</td><td>Granular MoE</td></tr>
                <tr><td>Switch Transformer</td><td>1.6T</td><td>~12B</td><td>2048 (Top-1)</td></tr>
            </table>
        </section>

        <!-- ğŸ’» ì½”ë“œ ì˜ˆì œ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ’» ì½”ë“œ ì˜ˆì œ</h2>
            <div class="code-tabs">
                <button class="code-tab active" data-lang="python">Python</button>
            </div>
            <div class="code-block" data-lang="python">
                
                <pre><code class="language-python"># MoE ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ (Mixtral)
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Mixtral 8x7B Instruct ë¡œë“œ (ì•½ 90GB VRAM í•„ìš”)
model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",  # ì—¬ëŸ¬ GPUì— ìë™ ë¶„ì‚°
    load_in_4bit=True   # ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ (ì•½ 25GB)
)

# ì¶”ë¡ 
messages = [{"role": "user", "content": "MoE ì•„í‚¤í…ì²˜ì˜ ì¥ë‹¨ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}]
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to(model.device)

outputs = model.generate(
    inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

# ===== MoE ëª¨ë¸ ì„œë¹™ (vLLM) =====
# pip install vllm
from vllm import LLM, SamplingParams

# vLLMì€ MoE ì „ìš© ìµœì í™” ì§€ì›
llm = LLM(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    tensor_parallel_size=2,  # 2 GPU ë³‘ë ¬
    dtype="bfloat16"
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=512)

prompts = ["[INST] Pythonì—ì„œ async/awaitì˜ ì‘ë™ ì›ë¦¬ëŠ”? [/INST]"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)

# ===== MoE ë ˆì´ì–´ ê°„ë‹¨ êµ¬í˜„ =====
import torch.nn as nn
import torch.nn.functional as F

class SimpleMoE(nn.Module):
    def __init__(self, dim, num_experts=8, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k

        # Expert FFN ë„¤íŠ¸ì›Œí¬ë“¤
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, dim * 4),
                nn.GELU(),
                nn.Linear(dim * 4, dim)
            ) for _ in range(num_experts)
        ])

        # Gating Network (ë¼ìš°í„°)
        self.gate = nn.Linear(dim, num_experts)

    def forward(self, x):
        # ë¼ìš°í„°: ê° í† í°ì— ëŒ€í•´ Expert ì„ íƒ
        gate_logits = self.gate(x)  # (batch, seq, num_experts)

        # Top-K Expert ì„ íƒ
        weights, selected = torch.topk(gate_logits, self.top_k, dim=-1)
        weights = F.softmax(weights, dim=-1)

        # ì„ íƒëœ Expertë“¤ì˜ ì¶œë ¥ ê°€ì¤‘ í•©ì‚°
        output = torch.zeros_like(x)
        for i, expert in enumerate(self.experts):
            mask = (selected == i).any(dim=-1)
            if mask.any():
                expert_out = expert(x[mask])
                # í•´ë‹¹ Expertì˜ ê°€ì¤‘ì¹˜ ì ìš©
                for k in range(self.top_k):
                    k_mask = (selected[..., k] == i) & mask.unsqueeze(-1)
                    if k_mask.any():
                        output[k_mask[..., 0]] += weights[..., k:k+1][k_mask[..., 0]] * expert_out

        return output

# ì‚¬ìš©
moe = SimpleMoE(dim=512, num_experts=8, top_k=2)
x = torch.randn(2, 10, 512)  # (batch=2, seq=10, dim=512)
out = moe(x)
print(f"MoE ì¶œë ¥: {out.shape}")  # (2, 10, 512)</code></pre>
                <button class="copy-btn" onclick="copyCode(this)">ğŸ“‹ ë³µì‚¬</button>
            </div>
        </section>

        <!-- ğŸ—£ï¸ ì‹¤ë¬´ ëŒ€í™” ì˜ˆì‹œ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ—£ï¸ ì‹¤ë¬´ì—ì„œ ì´ë ‡ê²Œ ë§í•˜ì„¸ìš”</h2>
            <div class="conversation-examples">
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ëª¨ë¸ ì„ ì • íšŒì˜ì—ì„œ</div>
                    <blockquote class="conv-quote">"ë¹„ìš© íš¨ìœ¨ì„ ìœ„í•´ MoE ëª¨ë¸ì„ ê²€í† í•´ë´ìš”. Mixtral 8x7BëŠ” 45B íŒŒë¼ë¯¸í„°ì§€ë§Œ ì¶”ë¡  FLOPsëŠ” 12B Denseì™€ ë¹„ìŠ·í•´ìš”. ê°™ì€ GPU ì˜ˆì‚°ìœ¼ë¡œ ë” í° ëª¨ë¸ ìš©ëŸ‰ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ë©´ì ‘ì—ì„œ</div>
                    <blockquote class="conv-quote">"MoEëŠ” Sparse ì•„í‚¤í…ì²˜ë¡œ ì „ì²´ íŒŒë¼ë¯¸í„° ì¤‘ ì¼ë¶€ Expertë§Œ í™œì„±í™”í•©ë‹ˆë‹¤. Gating Networkê°€ ì…ë ¥ë³„ë¡œ ì í•©í•œ Expertë¥¼ ì„ íƒí•˜ì£ . ì—°ì‚°ëŸ‰ì€ ì¤„ì§€ë§Œ ë©”ëª¨ë¦¬ëŠ” ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œí•´ì•¼ í•´ì„œ ì¤„ì§€ ì•ŠìŠµë‹ˆë‹¤. í•™ìŠµ ì‹œ Load Balancing Lossë¡œ Expert í™œìš© ê· í˜•ì„ ë§ì¶”ëŠ” ê²Œ ì¤‘ìš”í•©ë‹ˆë‹¤."</blockquote>
                </div>
                <div class="conv-item">
                    <div class="conv-context">ğŸ’¬ ê¸°ìˆ  í† ë¡ ì—ì„œ</div>
                    <blockquote class="conv-quote">"Dense vs MoE íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ìˆì–´ìš”. DenseëŠ” êµ¬í˜„ì´ ë‹¨ìˆœí•˜ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ì§€ë§Œ ìŠ¤ì¼€ì¼ë§ í•œê³„ê°€ ìˆê³ , MoEëŠ” ì—°ì‚° íš¨ìœ¨ì´ ì¢‹ì§€ë§Œ Expert Parallelism ë¶„ì‚° ì²˜ë¦¬ê°€ í•„ìš”í•´ìš”. ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì—ì„  MoEê°€ ìœ ë¦¬í•©ë‹ˆë‹¤."</blockquote>
                </div>
            </div>
        </section>

        <!-- âš ï¸ ì£¼ì˜ì‚¬í•­ -->
        <section class="term-section">
            <h2 class="section-title">âš ï¸ í”í•œ ì‹¤ìˆ˜ & ì£¼ì˜ì‚¬í•­</h2>
            <div class="warning-list">
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ë©”ëª¨ë¦¬ ì ˆê° ê¸°ëŒ€</strong>
                        <p>MoEëŠ” ì—°ì‚°(FLOPs)ì„ ì¤„ì´ì§€ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. 8x7B MoEëŠ” ì—¬ì „íˆ 45B ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âŒ</span>
                    <div>
                        <strong>ë‹¨ì¼ GPU ì‹œë„</strong>
                        <p>ëŒ€í˜• MoE ëª¨ë¸ì€ ë‹¨ì¼ GPUì— ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. Tensor Parallelismì´ë‚˜ ì–‘ìí™”ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.</p>
                    </div>
                </div>
                <div class="warning-item">
                    <span class="warning-icon">âœ…</span>
                    <div>
                        <strong>ì˜¬ë°”ë¥¸ ì ‘ê·¼: ìµœì í™”ëœ ì„œë¹™</strong>
                        <p>vLLM, TensorRT-LLM ë“± MoE ìµœì í™”ë¥¼ ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. Expert Parallelismìœ¼ë¡œ ì—¬ëŸ¬ GPUì— Expertë¥¼ ë¶„ì‚°í•˜ë©´ íš¨ìœ¨ì ì…ë‹ˆë‹¤. 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- ğŸ”— ê´€ë ¨ ìš©ì–´ -->
        <section class="term-section">
            <h2 class="section-title">ğŸ”— ê´€ë ¨ ìš©ì–´</h2>
            <div class="related-terms">
                <a href="/ko/term/Mixture%20of%20Experts/" class="related-term-link">Mixture of Experts</a>
                <a href="/ko/term/Mistral/" class="related-term-link">Mistral</a>
                <a href="/ko/term/Transformer/" class="related-term-link">Transformer</a>
                <a href="/ko/term/LLM/" class="related-term-link">LLM</a>
                <a href="/ko/term/Model%20Pruning/" class="related-term-link">Model Pruning</a>
            </div>
        </section>

        <!-- ğŸ“š ë” ë°°ìš°ê¸° -->
        <section class="term-section">
            <h2 class="section-title">ğŸ“š ë” ë°°ìš°ê¸°</h2>
            <div class="learn-more">
                <a href="https://arxiv.org/abs/2101.03961" target="_blank" class="learn-link">ğŸ“„ Switch Transformers ë…¼ë¬¸ (Google)</a>
                <a href="https://huggingface.co/blog/moe" target="_blank" class="learn-link">ğŸ“ HuggingFace MoE íŠœí† ë¦¬ì–¼</a>
                <a href="https://mistral.ai/news/mixtral-of-experts/" target="_blank" class="learn-link">ğŸ’» Mixtral MoE ë°œí‘œ</a>
            </div>
        </section>
    </main>

    <!-- Footer -->
        <div id="kaitrust-footer"></div>

    <!-- Scripts -->
    <script>document.getElementById('currentYear').textContent = new Date().getFullYear();</script>
    <script>window.WIA_A11Y_CONFIG = { fabBottom: "38px", fabRight: "30px" };</script>
    <script src="https://wia.live/wia-a11y-toolkit/wia-a11y-toolkit.min.js"></script>
    <script src="/components/ask-ai/kaitrust-ai-modal.js"></script>
    <script src="/components/language-modal/wia-language-modal-211.js"></script>
    <script>
    function copyCode(btn) {
        const codeBlock = btn.parentElement.querySelector('code');
        navigator.clipboard.writeText(codeBlock.textContent).then(() => {
            btn.textContent = 'âœ… ë³µì‚¬ë¨!';
            setTimeout(() => btn.textContent = 'ğŸ“‹ ë³µì‚¬', 2000);
        });
    }
    </script>
<script src="/glossary/js/term-sections.js?v=20260129233538"></script>
    <script src="https://kaitrust.ai/components/site-kit/kaitrust-site-kit.js"></script>
    <script src="/kaitrust-i18n.js?v=20260129"></script>
</body>
</html>
